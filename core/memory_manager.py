"""
è®°å¿†ç®¡ç†å™¨ (Memory Manager)

è´Ÿè´£è®°å¿†çš„å­˜å‚¨ã€æ£€ç´¢ã€å½’æ¡£ã€åˆ é™¤ç­‰æ ¸å¿ƒæ“ä½œã€‚
ä» memory_logic.py æå–è€Œæ¥ï¼Œéµå¾ªå•ä¸€èŒè´£åŸåˆ™ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- ChromaDB å‘é‡åº“çš„å»¶è¿Ÿåˆå§‹åŒ–ä¸ç®¡ç†
- åŸå§‹æ¶ˆæ¯è®°å½•
- è®°å¿†å½’æ¡£ä¸æ€»ç»“ï¼ˆæŒ‰å¤©åˆ†ç»„ï¼‰
- è¯­ä¹‰æ£€ç´¢ï¼ˆæ”¯æŒå…³é”®è¯é‡æ’åºï¼‰
- è®°å¿†åˆ é™¤ä¸æ’¤é”€
- æ•°æ®å¯¼å‡ºï¼ˆå¤šæ ¼å¼æ”¯æŒï¼‰

ä¾èµ–ï¼š
- context: AstrBot API ä¸Šä¸‹æ–‡ï¼ˆç”¨äº LLM è°ƒç”¨ï¼‰
- config: æ’ä»¶é…ç½®
- db_manager: æ•°æ®åº“ç®¡ç†å™¨
- profile_manager: ç”¨æˆ·ç”»åƒç®¡ç†å™¨ï¼ˆç”¨äºå®æ—¶æ›´æ–°ç”»åƒï¼‰
"""

import chromadb
import os
import uuid
import json
import re
import asyncio
import time
import datetime
from collections import Counter
from concurrent.futures import ThreadPoolExecutor
from astrbot.api import logger

# é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
_CHINESE_PATTERN = re.compile(r'[\u4e00-\u9fa5]')
_ENGLISH_WORD_PATTERN = re.compile(r"[a-z0-9]+(?:'[a-z0-9]+)?")
_CHINESE_BLOCK_PATTERN = re.compile(r"[\u4e00-\u9fa5]+")


class MemoryManager:
    """è®°å¿†ç®¡ç†å™¨"""
    
    def __init__(self, context, config, data_dir, executor, db_manager, profile_manager=None):
        """
        åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨
        
        Args:
            context: AstrBot API ä¸Šä¸‹æ–‡å¯¹è±¡
            config: æ’ä»¶é…ç½®å­—å…¸
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            executor: ThreadPoolExecutor å®ä¾‹
            db_manager: DatabaseManager å®ä¾‹
            profile_manager: ProfileManager å®ä¾‹ï¼ˆå¯é€‰ï¼Œç”¨äºå®æ—¶ç”»åƒæ›´æ–°ï¼‰
        """
        self.context = context
        self.config = config
        self.data_dir = data_dir
        self.executor = executor
        self.db = db_manager
        self.profile_manager = profile_manager
        
        # ChromaDB å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆé¿å…æ„é€ å‡½æ•°é˜»å¡ï¼‰
        self.chroma_path = os.path.join(self.data_dir, "engram_chroma")
        self.chroma_client = None
        self.collection = None
        self._chroma_init_lock = asyncio.Lock()
        self._chroma_initialized = False
        
        # å†…å­˜ä¸­è®°å½•æœ€åèŠå¤©æ—¶é—´ï¼ˆå¸¦è‡ªåŠ¨æ¸…ç†æœºåˆ¶ï¼‰
        self.last_chat_time = {}     # {user_id: timestamp}
        self.unsaved_msg_count = {}  # {user_id: count}
        self._max_inactive_users = 100  # æœ€å¤§ç¼“å­˜ç”¨æˆ·æ•°
        self._inactive_threshold = 7 * 24 * 3600  # 7å¤©æ— æ´»åŠ¨åˆ™æ¸…ç†
        
        # æ’¤é”€åˆ é™¤ç¼“å­˜ï¼š{user_id: [æœ€è¿‘åˆ é™¤çš„è®°å¿†åˆ—è¡¨]}
        self._delete_history = {}  # æ¯ä¸ªç”¨æˆ·ä¿ç•™æœ€è¿‘3æ¬¡åˆ é™¤
        self._max_undo_history = 3
        
        self._is_shutdown = False
    
    def shutdown(self):
        """å…³é—­è®°å¿†ç®¡ç†å™¨"""
        self._is_shutdown = True
    
    # ========== ChromaDB ç®¡ç† ==========
    
    async def _ensure_chroma_initialized(self):
        """ç¡®ä¿ ChromaDB å·²åˆå§‹åŒ–ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…æ„é€ å‡½æ•°é˜»å¡ï¼‰"""
        if self._chroma_initialized:
            return
        
        async with self._chroma_init_lock:
            # åŒé‡æ£€æŸ¥
            if self._chroma_initialized:
                return
            
            # åœ¨çº¿ç¨‹æ± ä¸­åˆå§‹åŒ– ChromaDBï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
            loop = asyncio.get_event_loop()
            
            def _init_chroma():
                client = chromadb.PersistentClient(path=self.chroma_path)
                collection = client.get_or_create_collection(name="long_term_memories")
                return client, collection
            
            try:
                self.chroma_client, self.collection = await loop.run_in_executor(
                    self.executor, _init_chroma
                )
                self._chroma_initialized = True
                logger.info("Engram: ChromaDB initialized successfully")
            except Exception as e:
                logger.error(f"Engram: Failed to initialize ChromaDB: {e}")
                raise
    
    # ========== è¾…åŠ©æ–¹æ³• ==========
    
    def _cleanup_inactive_users(self):
        """æ¸…ç†é•¿æœŸä¸æ´»è·ƒçš„ç”¨æˆ·ç¼“å­˜ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼"""
        now_ts = time.time()
        
        # æ‰¾å‡ºæ‰€æœ‰è¶…è¿‡é˜ˆå€¼çš„ä¸æ´»è·ƒç”¨æˆ·
        inactive_users = [
            user_id for user_id, last_time in self.last_chat_time.items()
            if now_ts - last_time > self._inactive_threshold
        ]
        
        # æ¸…ç†ä¸æ´»è·ƒç”¨æˆ·ï¼ˆä½†åªæœ‰åœ¨å·²å½’æ¡£åæ‰æ¸…ç†ï¼‰
        for user_id in inactive_users:
            if self.unsaved_msg_count.get(user_id, 0) == 0:
                self.last_chat_time.pop(user_id, None)
                self.unsaved_msg_count.pop(user_id, None)
        
        # å¦‚æœç”¨æˆ·æ•°ä»ç„¶è¿‡å¤šï¼ŒæŒ‰æœ€åæ´»è·ƒæ—¶é—´æ’åºï¼Œä¿ç•™æœ€è¿‘çš„
        if len(self.last_chat_time) > self._max_inactive_users:
            sorted_users = sorted(self.last_chat_time.items(), key=lambda x: x[1], reverse=True)
            users_to_keep = set(u[0] for u in sorted_users[:self._max_inactive_users])
            
            for user_id in list(self.last_chat_time.keys()):
                if user_id not in users_to_keep and self.unsaved_msg_count.get(user_id, 0) == 0:
                    self.last_chat_time.pop(user_id, None)
                    self.unsaved_msg_count.pop(user_id, None)
    
    @staticmethod
    def _ensure_datetime(timestamp):
        """
        ç¡®ä¿æ—¶é—´æˆ³æ˜¯ datetime å¯¹è±¡ã€‚
        å¦‚æœæ˜¯æ•´æ•°æˆ–æµ®ç‚¹æ•°ï¼ˆUnix æ—¶é—´æˆ³ï¼‰ï¼Œåˆ™è½¬æ¢ä¸º datetime å¯¹è±¡ã€‚
        """
        if isinstance(timestamp, (int, float)):
            return datetime.datetime.fromtimestamp(timestamp)
        return timestamp
    
    def _is_valid_message_content(self, content: str) -> bool:
        """
        ç»Ÿä¸€çš„æ¶ˆæ¯å†…å®¹è¿‡æ»¤é€»è¾‘ï¼Œç”¨äºåˆ¤æ–­æ¶ˆæ¯æ˜¯å¦åº”è¢«çº³å…¥å½’æ¡£/æ£€ç´¢ã€‚
        
        è¿‡æ»¤è§„åˆ™ï¼š
        1. ä»¥é…ç½®çš„æŒ‡ä»¤å‰ç¼€å¼€å¤´çš„æ¶ˆæ¯
        2. å¸¦ä¸‹åˆ’çº¿ä¸”æ— ç©ºæ ¼çš„å†…éƒ¨æŒ‡ä»¤
        3. ä¸­æ–‡å­—ç¬¦ä¸è¶³2ä¸ªä¸”æ€»é•¿åº¦ä¸è¶³10çš„çŸ­æ¶ˆæ¯
        
        è¿”å› True è¡¨ç¤ºæ¶ˆæ¯æœ‰æ•ˆï¼ŒFalse è¡¨ç¤ºåº”è¢«è¿‡æ»¤ã€‚
        """
        content = content.strip()
        
        # 1. è¿‡æ»¤ä»¥é…ç½®çš„æŒ‡ä»¤å‰ç¼€å¼€å¤´çš„æ¶ˆæ¯
        if self.config.get("enable_command_filter", True):
            command_prefixes = self.config.get("command_prefixes", ["/", "!", "#", "~"])
            if isinstance(command_prefixes, str):
                command_prefixes = [command_prefixes]
            command_prefixes = [str(p) for p in command_prefixes if str(p)]
            if command_prefixes and content.startswith(tuple(command_prefixes)):
                return False
        
        # 2. ä¸“é—¨æ¸…æ´—å¸¦ä¸‹åˆ’çº¿çš„å†…éƒ¨æŒ‡ä»¤
        if "_" in content and " " not in content:
            return False
        
        # 3. ç»Ÿè®¡ä¸­æ–‡æ•°é‡æˆ–æ£€æŸ¥æ€»é•¿åº¦
        chinese_chars = _CHINESE_PATTERN.findall(content)
        if len(chinese_chars) < 2 and len(content) < 10:
            return False
        
        return True

    def _generate_query_keywords(self, query: str):
        """ç”Ÿæˆä¸­è‹±æ··åˆå…³é”®è¯ï¼šè‹±æ–‡æŒ‰è¯åˆ‡åˆ†ï¼Œä¸­æ–‡æŒ‰ 2~4 gram åˆ‡åˆ†ã€‚"""
        min_n = max(2, int(self.config.get("keyword_ngram_min", 2)))
        max_n = max(min_n, int(self.config.get("keyword_ngram_max", 4)))
        max_n = min(max_n, 6)  # é˜²å¾¡æ€§ä¸Šé™ï¼Œé¿å…æç«¯é…ç½®å¯¼è‡´ç»„åˆçˆ†ç‚¸

        common_stopwords = {
            "a", "an", "the", "to", "of", "in", "on", "at", "is", "are", "i", "you", "he", "she", "it",
            "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", "è¿™", "é‚£", "äº†", "å•Š", "å‘€", "å—", "å‘¢", "å§", "å’Œ", "ä¸", "åŠ", "å°±", "ä¹Ÿ"
        }
        protected_tokens = {"ai", "ml", "db", "go", "c", "r"}

        english_tokens = _ENGLISH_WORD_PATTERN.findall(query.lower())
        query_keywords = set()

        for token in english_tokens:
            if not token:
                continue
            if len(token) <= 1 and token not in protected_tokens:
                continue
            if token in common_stopwords:
                continue
            query_keywords.add(token)

        chinese_blocks = _CHINESE_BLOCK_PATTERN.findall(query)
        for block in chinese_blocks:
            block_len = len(block)
            if block_len == 0:
                continue
            for n in range(min_n, max_n + 1):
                if block_len < n:
                    continue
                for i in range(0, block_len - n + 1):
                    gram = block[i:i + n]
                    if gram in common_stopwords:
                        continue
                    query_keywords.add(gram)

        return query_keywords

    def _count_keyword_matches(self, keyword: str, summary_tokens_en, summary_ngrams_zh):
        """è¾¹ç•Œæ„ŸçŸ¥åŒ¹é…ï¼šè‹±æ–‡æŒ‰è¯è¾¹ç•Œï¼Œä¸­æ–‡æŒ‰ n-gram ç²¾ç¡®è®¡æ•°ã€‚"""
        if not keyword:
            return 0

        if _CHINESE_PATTERN.search(keyword):
            return summary_ngrams_zh.get(keyword, 0)

        return summary_tokens_en.get(keyword.lower(), 0)

    def _calc_keyword_score(self, query: str, summary: str, corpus_stats: dict):
        """è®¡ç®—å…³é”®è¯å¾—åˆ†ï¼ˆè¾¹ç•Œæ„ŸçŸ¥åŒ¹é… + è¿‘ä¼¼ IDFï¼‰ã€‚"""
        query_keywords = self._generate_query_keywords(query)
        if not query_keywords or not summary:
            return 0.0, query_keywords

        summary_lower = summary.lower()
        summary_tokens_en = Counter(_ENGLISH_WORD_PATTERN.findall(summary_lower))

        min_n = max(2, int(self.config.get("keyword_ngram_min", 2)))
        max_n = max(min_n, int(self.config.get("keyword_ngram_max", 4)))
        max_n = min(max_n, 6)

        summary_ngrams_zh = Counter()
        chinese_blocks = _CHINESE_BLOCK_PATTERN.findall(summary)
        for block in chinese_blocks:
            block_len = len(block)
            for n in range(min_n, max_n + 1):
                if block_len < n:
                    continue
                for i in range(0, block_len - n + 1):
                    summary_ngrams_zh[block[i:i + n]] += 1

        matched_tf_sum = 0
        doc_len = max(1, len(summary_tokens_en) + sum(summary_ngrams_zh.values()))

        _bm25_k1 = 1.2
        _bm25_b = 0.75
        _avg_doc_len = 80

        keyword_score = 0.0
        total_docs = max(1, int(corpus_stats.get("total_docs", 1)))
        keyword_df = corpus_stats.get("keyword_doc_freq", {})

        for keyword in query_keywords:
            tf = self._count_keyword_matches(keyword, summary_tokens_en, summary_ngrams_zh)
            if tf <= 0:
                continue

            matched_tf_sum += tf
            norm_tf = (tf * (_bm25_k1 + 1)) / (tf + _bm25_k1 * (1 - _bm25_b + _bm25_b * doc_len / _avg_doc_len))

            # ç¨€æœ‰è¯æå‡ï¼ˆè¿‘ä¼¼ IDFï¼‰ï¼šå‡ºç°è¶Šå°‘ï¼Œæƒé‡è¶Šé«˜
            df = keyword_df.get(keyword, 0)
            idf = 1.0 + ((total_docs + 1.0) / (df + 1.0))
            keyword_score += norm_tf * min(4.0, idf)

        coverage_bonus = min(1.5, matched_tf_sum / max(1, len(query_keywords)))
        return keyword_score * (1.0 + 0.15 * coverage_bonus), query_keywords
    
    def _calc_keyword_score_legacy(self, query_keywords, summary: str):
        """æ—§ç‰ˆå…³é”®è¯æ‰“åˆ†ï¼šå­ä¸²åŒ¹é… + BM25 é£æ ¼ TF é¥±å’Œã€‚"""
        if not query_keywords or not summary:
            return 0.0

        _bm25_k1 = 1.2
        _bm25_b = 0.75
        _avg_doc_len = 80

        score = 0.0
        summary_lower = summary.lower()
        doc_len = max(1, len(summary_lower))

        for keyword in query_keywords:
            if keyword in summary_lower:
                tf = summary_lower.count(keyword)
                norm_tf = (tf * (_bm25_k1 + 1)) / (tf + _bm25_k1 * (1 - _bm25_b + _bm25_b * doc_len / _avg_doc_len))
                keyword_weight = max(1.0, min(3.0, len(keyword) / 2.0))
                score += norm_tf * keyword_weight

        return score

    # ========== æ¶ˆæ¯è®°å½• ==========
    
    async def record_message(self, user_id, session_id, role, content, msg_type="text", user_name=None):
        """è®°å½•åŸå§‹æ¶ˆæ¯"""
        msg_uuid = str(uuid.uuid4())
        
        # å¼‚æ­¥ä¿å­˜åˆ° SQLite
        loop = asyncio.get_event_loop()
        params = {
            "uuid": msg_uuid,
            "session_id": session_id,
            "user_id": user_id,
            "user_name": user_name,
            "role": role,
            "content": content,
            "msg_type": msg_type,
            "timestamp": datetime.datetime.now()
        }
        await loop.run_in_executor(self.executor, lambda: self.db.save_raw_memory(**params))
        
        # æ›´æ–°è®°å½•
        if role == "user":
            self.last_chat_time[user_id] = datetime.datetime.now().timestamp()
            self.unsaved_msg_count[user_id] = self.unsaved_msg_count.get(user_id, 0) + 1
    
    # ========== è®°å¿†å½’æ¡£ä¸æ€»ç»“ ==========
    
    async def check_and_summarize(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œç§èŠå½’æ¡£ï¼ˆç”»åƒæ›´æ–°ç”±ç‹¬ç«‹è°ƒåº¦å™¨å¤„ç†ï¼‰"""
        now_ts = datetime.datetime.now().timestamp()
        timeout = self.config.get("private_memory_timeout", 1800)
        min_count = self.config.get("min_msg_count", 3)
        
        for user_id, last_time in list(self.last_chat_time.items()):
            if now_ts - last_time > timeout and self.unsaved_msg_count.get(user_id, 0) >= min_count:
                # è§¦å‘è®°å¿†å½’æ¡£
                await self._summarize_private_chat(user_id)
                self.unsaved_msg_count[user_id] = 0
        
        # å®šæœŸæ¸…ç†ä¸æ´»è·ƒç”¨æˆ·ç¼“å­˜ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
        self._cleanup_inactive_users()
    
    async def _summarize_private_chat(self, user_id):
        """å¯¹ç§èŠè¿›è¡Œæ€»ç»“å¹¶å­˜å…¥é•¿æœŸè®°å¿†ï¼ˆæŒ‰å¤©åˆ†ç»„å¤„ç†ï¼‰"""
        from itertools import groupby
        
        # 1. è·å–æœªå½’æ¡£çš„åŸå§‹æ¶ˆæ¯
        loop = asyncio.get_event_loop()
        # è·å–æ‰€æœ‰æœªå½’æ¡£æ¶ˆæ¯ï¼Œä¸è®¾é™åˆ¶
        raw_msgs = await loop.run_in_executor(self.executor, lambda: self.db.get_unarchived_raw(user_id, limit=None))
        if not raw_msgs:
            return
        
        # æŒ‰æ—¶é—´æ­£åºæ’åˆ—ï¼ˆæ•°æ®åº“è¿”å›çš„æ˜¯å€’åºï¼‰
        raw_msgs.reverse()
        
        # è®¡ç®—å›æº¯æˆªæ­¢æ—¶é—´
        max_days = self.config.get("max_history_days", 0)
        cutoff_date = None
        if max_days > 0:
            cutoff_date = (datetime.datetime.now() - datetime.timedelta(days=max_days)).date()
        
        # æŒ‰æ—¥æœŸåˆ†ç»„
        def get_date_key(m):
            timestamp = m.timestamp
            # å¤„ç†æ—¶é—´æˆ³å¯èƒ½æ˜¯æ•´æ•°æˆ–æµ®ç‚¹æ•°çš„æƒ…å†µ
            if isinstance(timestamp, (int, float)):
                timestamp = datetime.datetime.fromtimestamp(timestamp)
            return timestamp.date()
            
        # ä»…æŸ¥è¯¢ä¸€æ¬¡æœ€è¿‘çš„è®°å¿†ç´¢å¼•ï¼Œæ„å»ºæ–°æ‰¹æ¬¡çš„é“¾è¡¨
        last_index = await loop.run_in_executor(self.executor, self.db.get_last_memory_index, user_id)
        prev_index_id = last_index.index_id if last_index else None

        batch_add = {
            "ids": [],
            "documents": [],
            "metadatas": []
        }
        index_params_list = []
        archive_uuids_forced = []
        archive_uuids_summarized = []

        for date_key, group in groupby(raw_msgs, key=get_date_key):
            # å°† group è½¬ä¸ºåˆ—è¡¨ï¼Œå› ä¸º groupby çš„è¿­ä»£å™¨åªèƒ½ç”¨ä¸€æ¬¡
            group_msgs = list(group)
            ref_uuids = [m.uuid for m in group_msgs]
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡å›æº¯å¤©æ•°é™åˆ¶
            if cutoff_date and date_key < cutoff_date:
                # è¶…è¿‡é™åˆ¶ï¼Œç›´æ¥æ ‡è®°ä¸ºå·²å½’æ¡£ï¼Œä¸è¿›è¡Œæ€»ç»“
                archive_uuids_forced.extend(ref_uuids)
                continue
                
            summary_result = await self._process_single_summary_batch(user_id, group_msgs, date_key)
            if not summary_result:
                continue

            summary = summary_result.get("summary")
            if not summary:
                if summary_result.get("archive", False):
                    archive_uuids_forced.extend(summary_result.get("ref_uuids", ref_uuids))
                continue

            created_at = summary_result["created_at"]
            ref_uuids = summary_result["ref_uuids"]

            index_id = str(uuid.uuid4())
            ai_name = self.config.get("ai_name", "åŠ©æ‰‹")
            batch_add["ids"].append(index_id)
            batch_add["documents"].append(summary)
            batch_add["metadatas"].append({
                "user_id": user_id,
                "source_type": "private",
                "created_at": created_at.strftime("%Y-%m-%d %H:%M:%S"),
                "ai_name": ai_name
            })

            index_params_list.append({
                "index_id": index_id,
                "summary": summary,
                "ref_uuids": json.dumps(ref_uuids),
                "prev_index_id": prev_index_id,
                "source_type": "private",
                "user_id": user_id,
                "created_at": created_at
            })
            prev_index_id = index_id
            archive_uuids_summarized.extend(ref_uuids)

        # å…ˆå½’æ¡£æ— éœ€æ€»ç»“çš„æ¶ˆæ¯
        if archive_uuids_forced:
            await loop.run_in_executor(self.executor, self.db.mark_as_archived, archive_uuids_forced)

        if not batch_add["ids"]:
            return

        max_retries = 3
        retry_delay = 2
        for attempt in range(1, max_retries + 1):
            try:
                # ç¡®ä¿ ChromaDB å·²åˆå§‹åŒ–
                await self._ensure_chroma_initialized()
                # æ‰¹é‡å†™å…¥å‘é‡æ•°æ®
                await loop.run_in_executor(self.executor, lambda: self.collection.add(**batch_add))
                logger.info(
                    "Engram: Batch add %d memories for user %s",
                    len(batch_add["ids"]),
                    user_id
                )
                break
            except Exception as e:
                if attempt >= max_retries:
                    logger.error(f"Save summarization error: {e}")
                    return
                logger.warning(
                    "Engram: Batch add failed (attempt %d/%d), retrying in %ss: %s",
                    attempt,
                    max_retries,
                    retry_delay,
                    e
                )
                await asyncio.sleep(retry_delay)
                retry_delay *= 2

        # æ‰¹é‡å†™å…¥ç´¢å¼•ï¼ˆé€æ¡å†™å…¥ SQLiteï¼‰
        for index_params in index_params_list:
            await loop.run_in_executor(self.executor, lambda p=index_params: self.db.save_memory_index(**p))

        # å½’æ¡£å·²æ€»ç»“çš„æ¶ˆæ¯
        if archive_uuids_summarized:
            await loop.run_in_executor(self.executor, self.db.mark_as_archived, archive_uuids_summarized)
    
    async def _process_single_summary_batch(self, user_id, raw_msgs, date_key):
        """å¤„ç†å•æ‰¹æ¬¡ï¼ˆå•æ—¥ï¼‰æ¶ˆæ¯çš„æ€»ç»“"""
        # ä½¿ç”¨å…¬å…±è¿‡æ»¤æ–¹æ³•
        filtered_msgs = [m for m in raw_msgs if self._is_valid_message_content(m.content)]
        
        loop = asyncio.get_event_loop()
        
        if not filtered_msgs:
            # å¦‚æœæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ¶ˆæ¯ï¼Œä¹Ÿæ ‡è®°åŸæœ¬çš„æ‰€æœ‰æ¶ˆæ¯ä¸ºå·²å½’æ¡£
            ref_uuids = [m.uuid for m in raw_msgs]
            return {
                "summary": None,
                "created_at": None,
                "ref_uuids": ref_uuids,
                "archive": True
            }

        # æ„é€ å¯¹è¯æ–‡æœ¬
        chat_lines = [f"ã€æ—¥æœŸï¼š{date_key.strftime('%Y-%m-%d')}ã€‘"]
        for m in filtered_msgs:
            # ç¡®ä¿æ—¶é—´æˆ³æ˜¯ datetime å¯¹è±¡
            ts = self._ensure_datetime(m.timestamp)
            time_str = ts.strftime("%H:%M")
            name = m.user_name if m.role == "user" and m.user_name else m.role
            chat_lines.append(f"[{time_str}] {name}: {m.content}")
        chat_text = "\n".join(chat_lines)
        
        # 2. è°ƒç”¨ LLM æ€»ç»“
        # ä»é…ç½®è·å–æç¤ºè¯æ¨¡æ¿å¹¶æ›¿æ¢å ä½ç¬¦
        custom_prompt = self.config.get("summarize_prompt")
        ai_name = self.config.get("ai_name")
        prompt = custom_prompt.replace("{{chat_text}}", chat_text).replace("{{ai_name}}", ai_name)
        
        max_retries = 3
        retry_delay = 2
        full_content = ""
        
        for attempt in range(max_retries):
            try:
                # è·å–æŒ‡å®šçš„æ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
                summarize_model = self.config.get("summarize_model", "").strip()
                if summarize_model:
                    provider = self.context.get_provider_by_id(summarize_model)
                    if not provider:
                        provider = self.context.get_using_provider()
                else:
                    provider = self.context.get_using_provider()

                if not provider:
                    break
                    
                resp = await provider.text_chat(prompt=prompt)
                full_content = resp.completion_text
                
                if full_content and len(full_content) >= 5:
                    break # æˆåŠŸè·å–æ€»ç»“
                
                logger.warning(f"Summarization attempt {attempt + 1} produced empty or too short result.")
            except Exception as e:
                logger.error(f"Summarization attempt {attempt + 1} error: {e}")
            
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
        
        if not full_content or len(full_content) < 5:
            logger.error(f"Failed to summarize chat for user {user_id} after {max_retries} attempts.")
            return None

        # æ€»ç»“ä»…ç”¨äºå½’æ¡£ï¼Œä¸åœ¨æ­¤å¤„åšç”»åƒæ›´æ–°
        summary = full_content

        ref_uuids = [m.uuid for m in raw_msgs]
        created_at = self._ensure_datetime(raw_msgs[-1].timestamp)

        return {
            "summary": summary,
            "created_at": created_at,
            "ref_uuids": ref_uuids,
            "archive": False
        }

    async def summarize_all_users(self):
        """å¼ºåˆ¶å½’æ¡£æ‰€æœ‰ç”¨æˆ·çš„æœªå½’æ¡£æ¶ˆæ¯"""
        loop = asyncio.get_event_loop()
        user_ids = await loop.run_in_executor(self.executor, self.db.get_all_user_ids)
        if not user_ids:
            return 0

        summarized = 0
        for uid in user_ids:
            if self._is_shutdown or getattr(self.executor, "_shutdown", False):
                logger.debug("Engram: Global summarize aborted due to shutdown")
                break

            # è·³è¿‡ç©ºå€¼æˆ–ç³»ç»Ÿå†…ç½®è´¦å·
            if uid is None:
                continue
            uid_str = str(uid).lower()
            if uid_str in {"system", "astrbot"}:
                continue

            try:
                await self._summarize_private_chat(uid)
                summarized += 1
            except Exception as e:
                logger.error(f"Engram: Force summarize failed for {uid}: {e}")
        return summarized
    
    # ========== è®°å¿†æ£€ç´¢ ==========
    
    async def retrieve_memories(self, user_id, query, limit=3):
        """æ£€ç´¢ç›¸å…³è®°å¿†å¹¶è¿”å›åŸæ–‡æ‘˜è¦åŠèƒŒæ™¯ï¼ˆåŸºäºæ—¶é—´é“¾ï¼‰ï¼Œä½¿ç”¨å…³é”®è¯é‡æ’åºæå‡ç²¾ç¡®åŒ¹é…"""
        # ç¡®ä¿ ChromaDB å·²åˆå§‹åŒ–
        await self._ensure_chroma_initialized()
        
        loop = asyncio.get_event_loop()
        
        # 1. ChromaDB æ£€ç´¢ï¼ˆå¤šå–ä¸€äº›ç»“æœä»¥ä¾¿è¿‡æ»¤å’Œé‡æ’åºåä»æœ‰è¶³å¤Ÿæ•°æ®ï¼‰
        query_params = {
            "query_texts": [query],
            "n_results": min(limit * 3, 15),  # å¤šå–ç»“æœä»¥ä¾¿é‡æ’åº
            "where": {"user_id": user_id}
        }
        results = await loop.run_in_executor(self.executor, lambda: self.collection.query(**query_params))
        
        if not results or not results['ids'] or not results['ids'][0]:
            return []
        
        # è·å–é…ç½®
        similarity_threshold = self.config.get("memory_similarity_threshold", 1.5)
        show_relevance_score = self.config.get("show_relevance_score", True)
        enable_keyword_boost = self.config.get("enable_keyword_boost", True)
        enable_ngram_keyword_rank = self.config.get("enable_ngram_keyword_rank", True)
        
        # è§£æå…³é”®è¯æƒé‡ï¼ˆæ–°æ ¼å¼ç›´æ¥æ˜¯æ•°å€¼å­—ç¬¦ä¸² "0.5"ï¼‰
        weight_config = self.config.get("keyword_boost_weight", "0.5")
        try:
            keyword_boost_weight = float(weight_config)
        except (ValueError, TypeError):
            # å‘åå…¼å®¹æ—§æ ¼å¼ "å‡è¡¡æ¨¡å¼ (0.5)"
            match = re.search(r'\(([\d.]+)\)', str(weight_config))
            keyword_boost_weight = float(match.group(1)) if match else 0.5
        
        # 2. é¢„å¤„ç†ç»“æœå¹¶è®¡ç®—å…³é”®è¯åŒ¹é…åº¦ï¼ˆè¾¹ç•Œæ„ŸçŸ¥ + è¿‘ä¼¼ IDFï¼‰
        distances = results.get('distances', [[]])[0] if 'distances' in results else []
        memory_data = []

        query_keywords = self._generate_query_keywords(query) if enable_ngram_keyword_rank else {k.lower() for k in re.split(r'[^\w]+', query) if k.strip()}

        # é¦–è½®æ”¶é›†å€™é€‰ï¼ˆå…ˆè¿‡æ»¤ç›¸ä¼¼åº¦ï¼‰
        candidate_memories = []
        for i in range(len(results['ids'][0])):
            distance = distances[i] if distances and i < len(distances) else float('inf')

            # è¿‡æ»¤ä½ç›¸å…³æ€§ç»“æœ
            if distance > similarity_threshold:
                logger.debug(f"Skipping memory with distance {distance:.3f} (threshold: {similarity_threshold})")
                continue

            candidate_memories.append({
                'index_id': results['ids'][0][i],
                'summary': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': distance
            })

        if enable_ngram_keyword_rank:
            # ç»Ÿè®¡å€™é€‰é›†ä¸­å„å…³é”®è¯æ–‡æ¡£é¢‘ç‡ï¼ˆç”¨äºç¨€æœ‰è¯æå‡ï¼‰
            keyword_doc_freq = Counter()
            min_n = max(2, int(self.config.get("keyword_ngram_min", 2)))
            max_n = max(min_n, int(self.config.get("keyword_ngram_max", 4)))
            max_n = min(max_n, 6)

            for item in candidate_memories:
                summary = item['summary']
                summary_tokens_en = Counter(_ENGLISH_WORD_PATTERN.findall(summary.lower()))
                summary_ngrams_zh = Counter()
                for block in _CHINESE_BLOCK_PATTERN.findall(summary):
                    for n in range(min_n, max_n + 1):
                        if len(block) < n:
                            continue
                        for pos in range(0, len(block) - n + 1):
                            summary_ngrams_zh[block[pos:pos + n]] += 1

                for keyword in query_keywords:
                    if self._count_keyword_matches(keyword, summary_tokens_en, summary_ngrams_zh) > 0:
                        keyword_doc_freq[keyword] += 1

            corpus_stats = {
                'total_docs': len(candidate_memories),
                'keyword_doc_freq': keyword_doc_freq
            }

            for item in candidate_memories:
                keyword_score, _ = self._calc_keyword_score(query, item['summary'], corpus_stats)
                item['keyword_score'] = keyword_score
                memory_data.append(item)
        else:
            for item in candidate_memories:
                item['keyword_score'] = self._calc_keyword_score_legacy(query_keywords, item['summary'])
                memory_data.append(item)
        
        # 3. RRF (Reciprocal Rank Fusion) èåˆæ’åº
        #    RRF_score(d) = w_v / (k + rank_vector(d)) + w_k / (k + rank_keyword(d))
        #    k=60 æ˜¯æ ‡å‡†å€¼ï¼Œkeyword_boost_weight æ§åˆ¶ä¸¤è·¯ä¿¡å·çš„æƒé‡æ¯”ä¾‹
        rrf_k = 60
        
        if enable_keyword_boost and query_keywords and len(memory_data) > 1:
            vector_w = 1.0 - keyword_boost_weight
            keyword_w = keyword_boost_weight
            
            # æŒ‰å‘é‡è·ç¦»æ’åï¼ˆè·ç¦»è¶Šå°æ’åè¶Šé å‰ï¼Œrank ä» 1 å¼€å§‹ï¼‰
            sorted_by_vector = sorted(range(len(memory_data)), key=lambda idx: memory_data[idx]['distance'])
            vector_rank = {idx: rank + 1 for rank, idx in enumerate(sorted_by_vector)}
            
            # æŒ‰å…³é”®è¯å¾—åˆ†æ’åï¼ˆå¾—åˆ†è¶Šé«˜æ’åè¶Šé å‰ï¼‰
            sorted_by_keyword = sorted(range(len(memory_data)), key=lambda idx: memory_data[idx]['keyword_score'], reverse=True)
            keyword_rank = {idx: rank + 1 for rank, idx in enumerate(sorted_by_keyword)}
            
            # è®¡ç®— RRF èåˆå¾—åˆ†
            for i, data in enumerate(memory_data):
                rrf_vector = vector_w / (rrf_k + vector_rank[i])
                rrf_keyword = keyword_w / (rrf_k + keyword_rank[i])
                data['rrf_score'] = rrf_vector + rrf_keyword
            
            # æŒ‰ RRF å¾—åˆ†æ’åºï¼ˆå¾—åˆ†è¶Šé«˜è¶Šé å‰ï¼‰
            memory_data.sort(key=lambda x: x['rrf_score'], reverse=True)
        else:
            # çº¯å‘é‡æ¨¡å¼æˆ–æ— å…³é”®è¯ï¼šé€€åŒ–ä¸ºæŒ‰è·ç¦»æ’åº
            for data in memory_data:
                data['rrf_score'] = max(0, 1 - data['distance'] / 2.0)
            memory_data.sort(key=lambda x: x['distance'])
        
        # 4. åªä¿ç•™å‰ limit æ¡
        memory_data = memory_data[:limit]
        # 5. æ„é€ å¸¦æ—¶é—´çº¿èƒŒæ™¯å’Œè¯„åˆ†çš„è®°å¿†æ–‡æœ¬ï¼ˆæ‰¹é‡æŸ¥è¯¢ï¼Œé¿å… N+1ï¼‰
        all_memories = []

        index_ids = [item['index_id'] for item in memory_data]
        index_map = await loop.run_in_executor(self.executor, self.db.get_memory_indices_by_ids, index_ids)

        enable_context_hint = self.config.get("enable_memory_context_hint", True)
        memory_context_window = max(0, int(self.config.get("memory_context_window", 1)))
        memory_context_window = min(memory_context_window, 5)

        # æŒ‰é…ç½®çª—å£æ”¶é›†å‰æƒ…ç´¢å¼• IDï¼ˆé“¾è·¯å±•å¼€ï¼‰
        context_prev_ids = set()
        context_prev_map = {}
        if enable_context_hint and memory_context_window > 0:
            frontier = [index_map[idx].prev_index_id for idx in index_ids if index_map.get(idx) and index_map[idx].prev_index_id]
            for _ in range(memory_context_window):
                if not frontier:
                    break
                prev_map = await loop.run_in_executor(self.executor, self.db.get_memory_indices_by_ids, frontier)
                next_frontier = []
                for pid, pidx in prev_map.items():
                    context_prev_ids.add(pid)
                    if pidx and pidx.prev_index_id:
                        next_frontier.append(pidx.prev_index_id)
                frontier = next_frontier
            if context_prev_ids:
                context_prev_map = await loop.run_in_executor(self.executor, self.db.get_memory_indices_by_ids, list(context_prev_ids))

        # æ‰¹é‡æ‹‰å–åŸæ–‡æ¶ˆæ¯
        uuid_lists = []
        for idx in index_ids:
            db_index = index_map.get(idx)
            if not db_index or not db_index.ref_uuids:
                continue
            try:
                uuids = json.loads(db_index.ref_uuids)
            except Exception:
                uuids = []
            if uuids:
                uuid_lists.append(uuids)

        raw_map = await loop.run_in_executor(self.executor, self.db.get_memories_by_uuids_map, uuid_lists) if uuid_lists else {}

        for data in memory_data:
            index_id = data['index_id']
            summary = data['summary']
            metadata = data['metadata']
            distance = data['distance']
            created_at = metadata.get("created_at", "æœªçŸ¥æ—¶é—´")

            quality_factor = max(0.0, 1.5 - distance) / 1.5
            if enable_keyword_boost and query_keywords and memory_data:
                best_rrf = memory_data[0].get('rrf_score', 1e-9)
                raw_percent = data.get('rrf_score', 0) / max(best_rrf, 1e-9) * 100
                relevance_percent = max(0, min(100, int(raw_percent * quality_factor)))
            else:
                relevance_percent = max(0, min(100, int((1 - distance / 2.0) * 100)))

            db_index = index_map.get(index_id)
            context_hint = ""
            if enable_context_hint and memory_context_window > 0 and db_index:
                snippets = []
                prev_id = db_index.prev_index_id
                for _ in range(memory_context_window):
                    if not prev_id:
                        break
                    prev_index = context_prev_map.get(prev_id)
                    if not prev_index:
                        break
                    snippet = prev_index.summary[:24].replace("\n", " ")
                    snippets.append(snippet)
                    prev_id = prev_index.prev_index_id
                if snippets:
                    timeline_text = " â†’ ".join(snippets)
                    if len(timeline_text) > 80:
                        timeline_text = timeline_text[:77] + "..."
                    context_hint = f"ï¼ˆå‰æƒ…æè¦ï¼š{timeline_text}ï¼‰"

            raw_preview = ""
            if db_index and db_index.ref_uuids:
                try:
                    uuids = tuple(sorted(str(u) for u in json.loads(db_index.ref_uuids) if u))
                except Exception:
                    uuids = tuple()
                raw_msgs = raw_map.get(uuids, []) if uuids else []
                filtered_raw = [m.content[:60] for m in raw_msgs if self._is_valid_message_content(m.content)][:2]
                if filtered_raw:
                    raw_preview = "\n   â”” ğŸ“„ ç›¸å…³åŸæ–‡ï¼š\n" + "\n".join([f"      {i+1}) {text}" for i, text in enumerate(filtered_raw)])

            short_id = index_id[:8]
            relevance_badge = f"ğŸ¯ {relevance_percent}% | " if show_relevance_score else ""
            all_memories.append(f"{relevance_badge}ğŸ†” {short_id} | â° {created_at}\nğŸ“ å½’æ¡£ï¼š{summary}{context_hint}{raw_preview}")
        # 6. Reinforceï¼šè¢«æˆåŠŸå¬å›çš„è®°å¿†å¢å¼º active_score
        reinforce_bonus = self.config.get("memory_reinforce_bonus", 20)
        if all_memories and reinforce_bonus > 0:
            for data in memory_data:
                try:
                    await loop.run_in_executor(
                        self.executor,
                        self.db.update_active_score,
                        data['index_id'],
                        reinforce_bonus
                    )
                except Exception as e:
                    logger.debug(f"Engram: Failed to reinforce memory {data['index_id'][:8]}: {e}")
            
        return all_memories
    
    async def get_memory_detail(self, user_id, sequence_num):
        """è·å–æŒ‡å®šåºå·è®°å¿†çš„å®Œæ•´åŸæ–‡è¯¦æƒ…"""
        loop = asyncio.get_event_loop()
        
        # 1. è·å–æœ€è¿‘çš„ N æ¡è®°å¿†ï¼ˆä¸ºäº†æ‰¾åˆ°å¯¹åº”çš„åºå·ï¼‰
        # å‡è®¾ç”¨æˆ·è¾“å…¥çš„åºå·æ˜¯åŸºäº mem_list çš„ï¼ˆæœ€æ–°çš„ä¸º 1ï¼‰
        limit = sequence_num + 2
        memories = await loop.run_in_executor(self.executor, self.db.get_memory_list, user_id, limit)
        
        if not memories or len(memories) < sequence_num:
            return None, "æ‰¾ä¸åˆ°è¯¥åºå·çš„è®°å¿†ï¼Œè¯·ç¡®è®¤åºå·æ˜¯å¦å­˜åœ¨ã€‚"
            
        # 2. é”å®šç›®æ ‡è®°å¿†
        target_memory = memories[sequence_num - 1]
        
        # 3. è§£æåŸæ–‡ UUID
        if not target_memory.ref_uuids:
            return target_memory, []
            
        uuids = json.loads(target_memory.ref_uuids)
        raw_msgs = await loop.run_in_executor(self.executor, self.db.get_memories_by_uuids, uuids)
        
        return target_memory, raw_msgs
    
    async def get_memory_detail_by_id(self, user_id, short_id):
        """
        æ ¹æ®è®°å¿† IDï¼ˆçŸ­ ID æˆ–å®Œæ•´ UUIDï¼‰è·å–è®°å¿†è¯¦æƒ…
        
        Args:
            user_id: ç”¨æˆ·ID
            short_id: è®°å¿†IDï¼ˆå¯ä»¥æ˜¯å‰8ä½çŸ­IDæˆ–å®Œæ•´UUIDï¼‰
            
        Returns:
            (memory_index, raw_msgs) æˆ– (None, error_message)
        """
        loop = asyncio.get_event_loop()
        
        # 1. æŸ¥æ‰¾åŒ¹é…çš„è®°å¿†ç´¢å¼•
        def _find_memory():
            with self.db.db.connection_context():
                from ..db_manager import MemoryIndex
                # å¦‚æœæ˜¯çŸ­IDï¼ˆ8ä½ï¼‰ï¼ŒæŸ¥æ‰¾åŒ¹é…çš„å®Œæ•´UUID
                if len(short_id) == 8:
                    query = MemoryIndex.select().where(
                        (MemoryIndex.user_id == user_id) &
                        (MemoryIndex.index_id.startswith(short_id))
                    )
                else:
                    # å®Œæ•´UUID
                    query = MemoryIndex.select().where(
                        (MemoryIndex.user_id == user_id) &
                        (MemoryIndex.index_id == short_id)
                    )
                return query.first()
        
        target_memory = await loop.run_in_executor(self.executor, _find_memory)
        
        if not target_memory:
            return None, f"æ‰¾ä¸åˆ° ID ä¸º {short_id} çš„è®°å¿†ï¼Œè¯·ç¡®è®¤ ID æ˜¯å¦æ­£ç¡®ã€‚"
        
        # 2. è§£æåŸæ–‡ UUID
        if not target_memory.ref_uuids:
            return target_memory, []
            
        uuids = json.loads(target_memory.ref_uuids)
        raw_msgs = await loop.run_in_executor(self.executor, self.db.get_memories_by_uuids, uuids)
        
        return target_memory, raw_msgs
    
    # ========== è®°å¿†åˆ é™¤ä¸æ’¤é”€ ==========
    
    async def delete_memory_by_sequence(self, user_id, sequence_num, delete_raw=False):
        """
        åˆ é™¤æŒ‡å®šåºå·çš„è®°å¿†ï¼ˆæ”¯æŒæ’¤é”€ï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            sequence_num: è®°å¿†åºå·ï¼ˆåŸºäº mem_list çš„åºå·ï¼Œæœ€æ–°çš„ä¸º 1ï¼‰
            delete_raw: æ˜¯å¦åŒæ—¶åˆ é™¤å…³è”çš„åŸå§‹æ¶ˆæ¯
            
        Returns:
            (success: bool, message: str, summary: str)
        """
        loop = asyncio.get_event_loop()
        
        # 1. è·å–ç›®æ ‡è®°å¿†
        limit = sequence_num + 2
        memories = await loop.run_in_executor(self.executor, self.db.get_memory_list, user_id, limit)
        
        if not memories or len(memories) < sequence_num:
            return False, "æ‰¾ä¸åˆ°è¯¥åºå·çš„è®°å¿†ï¼Œè¯·ç¡®è®¤åºå·æ˜¯å¦å­˜åœ¨ã€‚", ""
            
        target_memory = memories[sequence_num - 1]
        index_id = target_memory.index_id
        summary = target_memory.summary
        
        try:
            # ç¡®ä¿ ChromaDB å·²åˆå§‹åŒ–
            await self._ensure_chroma_initialized()
            
            # ä¿å­˜åˆ é™¤å‰çš„æ•°æ®ï¼ˆç”¨äºæ’¤é”€ï¼‰
            deleted_uuids = json.loads(target_memory.ref_uuids) if target_memory.ref_uuids else []
            
            # è·å–å‘é‡æ•°æ®ï¼ˆç”¨äºæ¢å¤ï¼‰
            vector_data = None
            try:
                chroma_result = await loop.run_in_executor(
                    self.executor,
                    lambda: self.collection.get(ids=[index_id], include=['embeddings', 'metadatas', 'documents'])
                )
                if chroma_result and chroma_result['ids']:
                    vector_data = {
                        'embedding': chroma_result['embeddings'][0] if chroma_result.get('embeddings') else None,
                        'metadata': chroma_result['metadatas'][0] if chroma_result.get('metadatas') else {},
                        'document': chroma_result['documents'][0] if chroma_result.get('documents') else summary
                    }
            except Exception as e:
                logger.debug(f"Failed to get vector data for backup: {e}")
            
            # åˆ›å»ºåˆ é™¤è®°å½•
            delete_record = {
                'index_id': index_id,
                'summary': summary,
                'ref_uuids': target_memory.ref_uuids,
                'prev_index_id': target_memory.prev_index_id,
                'source_type': target_memory.source_type,
                'user_id': user_id,
                'created_at': target_memory.created_at,
                'active_score': target_memory.active_score,
                'delete_raw': delete_raw,
                'deleted_uuids': deleted_uuids,
                'vector_data': vector_data
            }
            
            # ä¿å­˜åˆ°åˆ é™¤å†å²
            if user_id not in self._delete_history:
                self._delete_history[user_id] = []
            self._delete_history[user_id].insert(0, delete_record)
            # åªä¿ç•™æœ€è¿‘Næ¬¡åˆ é™¤
            self._delete_history[user_id] = self._delete_history[user_id][:self._max_undo_history]
            
            # 2. ä» ChromaDB åˆ é™¤å‘é‡æ•°æ®
            await loop.run_in_executor(self.executor, lambda: self.collection.delete(ids=[index_id]))
            
            # 3. å¦‚æœéœ€è¦ï¼Œåˆ é™¤å…³è”çš„åŸå§‹æ¶ˆæ¯
            if delete_raw and target_memory.ref_uuids:
                uuids = json.loads(target_memory.ref_uuids)
                await loop.run_in_executor(self.executor, self.db.delete_raw_memories_by_uuids, uuids)
            else:
                # ä¸åˆ é™¤åŸå§‹æ¶ˆæ¯æ—¶ï¼Œå°†å…¶æ ‡è®°ä¸ºæœªå½’æ¡£ï¼Œä»¥ä¾¿é‡æ–°æ€»ç»“
                if deleted_uuids:
                    def _mark_unarchived():
                        from ..db_manager import RawMemory
                        with self.db.db.connection_context():
                            RawMemory.update(is_archived=False).where(RawMemory.uuid << deleted_uuids).execute()
                    await loop.run_in_executor(self.executor, _mark_unarchived)
            
            # 4. ä» SQLite åˆ é™¤è®°å¿†ç´¢å¼•
            await loop.run_in_executor(self.executor, self.db.delete_memory_index, index_id)
            
            return True, "åˆ é™¤æˆåŠŸ", summary
            
        except Exception as e:
            logger.error(f"Delete memory error: {e}")
            return False, f"åˆ é™¤å¤±è´¥ï¼š{e}", summary
    
    async def undo_last_delete(self, user_id):
        """
        æ’¤é”€æœ€è¿‘ä¸€æ¬¡åˆ é™¤æ“ä½œ
        
        Args:
            user_id: ç”¨æˆ·ID
            
        Returns:
            (success: bool, message: str, summary: str)
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ é™¤å†å²
        if user_id not in self._delete_history or not self._delete_history[user_id]:
            return False, "æ²¡æœ‰å¯æ’¤é”€çš„åˆ é™¤æ“ä½œã€‚", ""
        
        # è·å–æœ€è¿‘çš„åˆ é™¤è®°å½•
        delete_record = self._delete_history[user_id].pop(0)
        
        loop = asyncio.get_event_loop()
        
        try:
            # 1. æ¢å¤ SQLite ä¸­çš„è®°å¿†ç´¢å¼•
            index_params = {
                'index_id': delete_record['index_id'],
                'summary': delete_record['summary'],
                'ref_uuids': delete_record['ref_uuids'],
                'prev_index_id': delete_record['prev_index_id'],
                'source_type': delete_record['source_type'],
                'user_id': delete_record['user_id'],
                'created_at': delete_record['created_at'],
                'active_score': delete_record.get('active_score', 100)
            }
            await loop.run_in_executor(self.executor, lambda: self.db.save_memory_index(**index_params))
            
            # ç¡®ä¿ ChromaDB å·²åˆå§‹åŒ–
            await self._ensure_chroma_initialized()
            
            # 2. æ¢å¤ ChromaDB ä¸­çš„å‘é‡æ•°æ®
            vector_data = delete_record.get('vector_data')
            if vector_data and vector_data.get('embedding'):
                # æœ‰å®Œæ•´çš„å‘é‡æ•°æ®ï¼Œç›´æ¥æ¢å¤
                add_params = {
                    'ids': [delete_record['index_id']],
                    'documents': [vector_data.get('document', delete_record['summary'])],
                    'metadatas': [vector_data.get('metadata', {'user_id': user_id})],
                    'embeddings': [vector_data['embedding']]
                }
                await loop.run_in_executor(self.executor, lambda: self.collection.add(**add_params))
            else:
                # æ²¡æœ‰å‘é‡æ•°æ®ï¼Œé‡æ–°ç”Ÿæˆ
                add_params = {
                    'ids': [delete_record['index_id']],
                    'documents': [delete_record['summary']],
                    'metadatas': [{
                        'user_id': user_id,
                        'source_type': delete_record['source_type'],
                        'created_at': delete_record['created_at'].strftime("%Y-%m-%d %H:%M:%S") if hasattr(delete_record['created_at'], 'strftime') else str(delete_record['created_at'])
                    }]
                }
                await loop.run_in_executor(self.executor, lambda: self.collection.add(**add_params))
            
            # 3. æ¢å¤åŸå§‹æ¶ˆæ¯çš„å½’æ¡£çŠ¶æ€
            if delete_record['deleted_uuids']:
                def _mark_archived():
                    from ..db_manager import RawMemory
                    with self.db.db.connection_context():
                        RawMemory.update(is_archived=True).where(
                            RawMemory.uuid << delete_record['deleted_uuids']
                        ).execute()
                try:
                    await loop.run_in_executor(self.executor, _mark_archived)
                except Exception as e:
                    logger.debug(f"Failed to restore raw messages archive status: {e}")
            
            return True, "æ’¤é”€æˆåŠŸ", delete_record['summary']
            
        except Exception as e:
            logger.error(f"Undo delete error: {e}")
            # æ¢å¤å¤±è´¥ï¼Œå°†è®°å½•æ”¾å›å†å²
            self._delete_history[user_id].insert(0, delete_record)
            return False, f"æ’¤é”€å¤±è´¥ï¼š{e}", delete_record['summary']
    
    async def delete_memory_by_id(self, user_id, short_id, delete_raw=False):
        """
        æ ¹æ®è®°å¿† IDï¼ˆçŸ­ ID æˆ–å®Œæ•´ UUIDï¼‰åˆ é™¤è®°å¿†
        
        Args:
            user_id: ç”¨æˆ·ID
            short_id: è®°å¿†IDï¼ˆå¯ä»¥æ˜¯å‰8ä½çŸ­IDæˆ–å®Œæ•´UUIDï¼‰
            delete_raw: æ˜¯å¦åŒæ—¶åˆ é™¤å…³è”çš„åŸå§‹æ¶ˆæ¯
            
        Returns:
            (success: bool, message: str, summary: str)
        """
        loop = asyncio.get_event_loop()
        
        # 1. æŸ¥æ‰¾åŒ¹é…çš„è®°å¿†ç´¢å¼•
        def _find_memory():
            with self.db.db.connection_context():
                from ..db_manager import MemoryIndex
                # å¦‚æœæ˜¯çŸ­IDï¼ˆ8ä½ï¼‰ï¼ŒæŸ¥æ‰¾åŒ¹é…çš„å®Œæ•´UUID
                if len(short_id) == 8:
                    query = MemoryIndex.select().where(
                        (MemoryIndex.user_id == user_id) &
                        (MemoryIndex.index_id.startswith(short_id))
                    )
                else:
                    # å®Œæ•´UUID
                    query = MemoryIndex.select().where(
                        (MemoryIndex.user_id == user_id) &
                        (MemoryIndex.index_id == short_id)
                    )
                return query.first()
        
        try:
            target_memory = await loop.run_in_executor(self.executor, _find_memory)
            
            if not target_memory:
                return False, f"æ‰¾ä¸åˆ° ID ä¸º {short_id} çš„è®°å¿†ï¼Œè¯·ç¡®è®¤ ID æ˜¯å¦æ­£ç¡®ã€‚", ""
            
            index_id = target_memory.index_id
            summary = target_memory.summary
            
            # ç¡®ä¿ ChromaDB å·²åˆå§‹åŒ–
            await self._ensure_chroma_initialized()
            
            # 2. ä» ChromaDB åˆ é™¤å‘é‡æ•°æ®
            await loop.run_in_executor(self.executor, lambda: self.collection.delete(ids=[index_id]))
            
            # 3. å¦‚æœéœ€è¦ï¼Œåˆ é™¤å…³è”çš„åŸå§‹æ¶ˆæ¯
            if delete_raw and target_memory.ref_uuids:
                uuids = json.loads(target_memory.ref_uuids)
                await loop.run_in_executor(self.executor, self.db.delete_raw_memories_by_uuids, uuids)
            
            # 4. ä» SQLite åˆ é™¤è®°å¿†ç´¢å¼•
            await loop.run_in_executor(self.executor, self.db.delete_memory_index, index_id)
            
            return True, "åˆ é™¤æˆåŠŸ", summary
            
        except Exception as e:
            logger.error(f"Delete memory by ID error: {e}")
            return False, f"åˆ é™¤å¤±è´¥ï¼š{e}", ""
    
    # ========== æ•°æ®å¯¼å‡º ==========
    
    async def export_raw_messages(self, user_id, format="jsonl", start_date=None, end_date=None, limit=None):
        """
        å¯¼å‡ºåŸå§‹æ¶ˆæ¯æ•°æ®ç”¨äºæ¨¡å‹å¾®è°ƒ
        
        Args:
            user_id: ç”¨æˆ·ID
            format: å¯¼å‡ºæ ¼å¼ (jsonl, json, txt)
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            limit: é™åˆ¶æ•°é‡
            
        Returns:
            (success: bool, data: str, stats: dict)
        """
        loop = asyncio.get_event_loop()
        
        try:
            # è·å–åŸå§‹æ¶ˆæ¯
            raw_msgs = await loop.run_in_executor(
                self.executor,
                self.db.get_all_raw_messages,
                user_id,
                start_date,
                end_date,
                limit
            )
            
            if not raw_msgs:
                return False, "æ²¡æœ‰æ‰¾åˆ°å¯å¯¼å‡ºçš„æ¶ˆæ¯", {}
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = await loop.run_in_executor(self.executor, self.db.get_message_stats, user_id)
            stats["exported"] = len(raw_msgs)
            
            # æ ¹æ®æ ¼å¼å¯¼å‡º
            if format == "jsonl":
                data = self._export_as_jsonl(raw_msgs)
            elif format == "json":
                data = self._export_as_json(raw_msgs)
            elif format == "txt":
                data = self._export_as_txt(raw_msgs)
            elif format == "alpaca":
                data = self._export_as_alpaca(raw_msgs)
            elif format == "sharegpt":
                data = self._export_as_sharegpt(raw_msgs)
            else:
                return False, f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼ï¼š{format}", {}
            
            return True, data, stats
            
        except Exception as e:
            logger.error(f"Export raw messages error: {e}")
            return False, f"å¯¼å‡ºå¤±è´¥ï¼š{e}", {}
    
    async def export_all_users_messages(self, format="jsonl", start_date=None, end_date=None, limit=None):
        """
        å¯¼å‡ºæ‰€æœ‰ç”¨æˆ·çš„åŸå§‹æ¶ˆæ¯æ•°æ®
        
        Args:
            format: å¯¼å‡ºæ ¼å¼ (jsonl, json, txt, alpaca, sharegpt)
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            limit: é™åˆ¶æ•°é‡
            
        Returns:
            (success: bool, data: str, stats: dict)
        """
        loop = asyncio.get_event_loop()
        
        try:
            # è·å–æ‰€æœ‰ç”¨æˆ·çš„æ¶ˆæ¯
            raw_msgs = await loop.run_in_executor(
                self.executor,
                self.db.get_all_users_messages,
                start_date,
                end_date,
                limit
            )
            
            if not raw_msgs:
                return False, "æ²¡æœ‰æ‰¾åˆ°å¯å¯¼å‡ºçš„æ¶ˆæ¯", {}
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = await loop.run_in_executor(self.executor, self.db.get_all_users_stats)
            stats["exported"] = len(raw_msgs)
            
            # æ ¹æ®æ ¼å¼å¯¼å‡º
            if format == "jsonl":
                data = self._export_as_jsonl(raw_msgs)
            elif format == "json":
                data = self._export_as_json(raw_msgs)
            elif format == "txt":
                data = self._export_as_txt(raw_msgs)
            elif format == "alpaca":
                data = self._export_as_alpaca(raw_msgs)
            elif format == "sharegpt":
                data = self._export_as_sharegpt(raw_msgs)
            else:
                return False, f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼ï¼š{format}", {}
            
            return True, data, stats
            
        except Exception as e:
            logger.error(f"Export all users messages error: {e}")
            return False, f"å¯¼å‡ºå¤±è´¥ï¼š{e}", {}
    
    def _export_as_jsonl(self, raw_msgs):
        """å¯¼å‡ºä¸º JSONL æ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰"""
        lines = []
        for msg in raw_msgs:
            if not self._is_valid_message_content(msg.content):
                continue
            ts = self._ensure_datetime(msg.timestamp)
            obj = {
                "role": "assistant" if msg.role == "assistant" else "user",
                "content": msg.content,
                "timestamp": ts.strftime("%Y-%m-%d %H:%M:%S"),
                "user_id": msg.user_id,
                "user_name": msg.user_name
            }
            lines.append(json.dumps(obj, ensure_ascii=False))
        return "\n".join(lines)
    
    def _export_as_json(self, raw_msgs):
        """å¯¼å‡ºä¸º JSON æ•°ç»„æ ¼å¼"""
        messages = []
        for msg in raw_msgs:
            if not self._is_valid_message_content(msg.content):
                continue
            ts = self._ensure_datetime(msg.timestamp)
            messages.append({
                "role": "assistant" if msg.role == "assistant" else "user",
                "content": msg.content,
                "timestamp": ts.strftime("%Y-%m-%d %H:%M:%S"),
                "user_id": msg.user_id,
                "user_name": msg.user_name
            })
        return json.dumps(messages, ensure_ascii=False, indent=2)
    
    def _export_as_txt(self, raw_msgs):
        """å¯¼å‡ºä¸ºçº¯æ–‡æœ¬æ ¼å¼"""
        lines = []
        for msg in raw_msgs:
            if not self._is_valid_message_content(msg.content):
                continue
            ts = self._ensure_datetime(msg.timestamp)
            role_name = "åŠ©æ‰‹" if msg.role == "assistant" else (msg.user_name or "ç”¨æˆ·")
            time_str = ts.strftime("%Y-%m-%d %H:%M:%S")
            lines.append(f"[{time_str}] {role_name}: {msg.content}")
        return "\n".join(lines)
    
    def _export_as_alpaca(self, raw_msgs):
        """å¯¼å‡ºä¸º Alpaca æ ¼å¼ï¼ˆç”¨äºå¾®è°ƒï¼‰"""
        conversations = []
        current_instruction = None
        
        for msg in raw_msgs:
            if not self._is_valid_message_content(msg.content):
                continue
                
            if msg.role == "user":
                current_instruction = msg.content
            elif msg.role == "assistant" and current_instruction:
                conversations.append({
                    "instruction": current_instruction,
                    "input": "",
                    "output": msg.content
                })
                current_instruction = None
        
        return json.dumps(conversations, ensure_ascii=False, indent=2)
    
    def _export_as_sharegpt(self, raw_msgs):
        """å¯¼å‡ºä¸º ShareGPT æ ¼å¼ï¼ˆç”¨äºå¾®è°ƒï¼‰"""
        conversations = []
        current_conversation = []
        
        for msg in raw_msgs:
            if not self._is_valid_message_content(msg.content):
                continue
            
            role = "gpt" if msg.role == "assistant" else "human"
            current_conversation.append({
                "from": role,
                "value": msg.content
            })
            
            # æ¯ä¸ªå¯¹è¯è½®æ¬¡ï¼ˆä¸€é—®ä¸€ç­”ï¼‰ä½œä¸ºä¸€ä¸ªå®Œæ•´å¯¹è¯
            if msg.role == "assistant" and len(current_conversation) >= 2:
                conversations.append({
                    "conversations": current_conversation.copy()
                })
                current_conversation = []
        
        return json.dumps(conversations, ensure_ascii=False, indent=2)
