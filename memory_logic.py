import chromadb
import os
import uuid
import json
import re
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor
from astrbot.api import logger
from .db_manager import DatabaseManager

# é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
_CHINESE_PATTERN = re.compile(r'[\u4e00-\u9fa5]')

class MemoryLogic:
    def __init__(self, context, config, data_dir):
        self.context = context
        self.config = config
        self.data_dir = data_dir
        os.makedirs(self.data_dir, exist_ok=True)
        
        self.db = DatabaseManager(self.data_dir)
        
        # ChromaDB å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆé¿å…æ„é€ å‡½æ•°é˜»å¡ï¼‰
        self.chroma_path = os.path.join(self.data_dir, "engram_chroma")
        self.chroma_client = None
        self.collection = None
        self._chroma_init_lock = asyncio.Lock()
        self._chroma_initialized = False
        
        # ç”¨æˆ·ç”»åƒè·¯å¾„
        self.profiles_dir = os.path.join(self.data_dir, "engram_personas")
        os.makedirs(self.profiles_dir, exist_ok=True)
        
        # çº¿ç¨‹æ± å¤„ç†æ•°æ®åº“å’Œå‘é‡åº“æ“ä½œ
        self.executor = ThreadPoolExecutor(max_workers=4)
        self._is_shutdown = False
        
        # å†…å­˜ä¸­è®°å½•æœ€åèŠå¤©æ—¶é—´ï¼ˆå¸¦è‡ªåŠ¨æ¸…ç†æœºåˆ¶ï¼‰
        self.last_chat_time = {}     # {user_id: timestamp}
        self.unsaved_msg_count = {}  # {user_id: count}
        self._max_inactive_users = 100  # æœ€å¤§ç¼“å­˜ç”¨æˆ·æ•°
        self._inactive_threshold = 7 * 24 * 3600  # 7å¤©æ— æ´»åŠ¨åˆ™æ¸…ç†
        
        # æ’¤é”€åˆ é™¤ç¼“å­˜ï¼š{user_id: [æœ€è¿‘åˆ é™¤çš„è®°å¿†åˆ—è¡¨]}
        self._delete_history = {}  # æ¯ä¸ªç”¨æˆ·ä¿ç•™æœ€è¿‘3æ¬¡åˆ é™¤
        self._max_undo_history = 3

    def shutdown(self):
        self._is_shutdown = True
        self.executor.shutdown(wait=False)
    
    async def _ensure_chroma_initialized(self):
        """ç¡®ä¿ ChromaDB å·²åˆå§‹åŒ–ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…æ„é€ å‡½æ•°é˜»å¡ï¼‰"""
        if self._chroma_initialized:
            return
        
        async with self._chroma_init_lock:
            # åŒé‡æ£€æŸ¥
            if self._chroma_initialized:
                return
            
            # åœ¨çº¿ç¨‹æ± ä¸­åˆå§‹åŒ– ChromaDBï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
            loop = asyncio.get_event_loop()
            
            def _init_chroma():
                client = chromadb.PersistentClient(path=self.chroma_path)
                collection = client.get_or_create_collection(name="long_term_memories")
                return client, collection
            
            try:
                self.chroma_client, self.collection = await loop.run_in_executor(
                    self.executor, _init_chroma
                )
                self._chroma_initialized = True
                logger.info("Engram: ChromaDB initialized successfully")
            except Exception as e:
                logger.error(f"Engram: Failed to initialize ChromaDB: {e}")
                raise

    def _cleanup_inactive_users(self):
        """æ¸…ç†é•¿æœŸä¸æ´»è·ƒçš„ç”¨æˆ·ç¼“å­˜ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼"""
        now_ts = time.time()
        
        # æ‰¾å‡ºæ‰€æœ‰è¶…è¿‡é˜ˆå€¼çš„ä¸æ´»è·ƒç”¨æˆ·
        inactive_users = [
            user_id for user_id, last_time in self.last_chat_time.items()
            if now_ts - last_time > self._inactive_threshold
        ]
        
        # æ¸…ç†ä¸æ´»è·ƒç”¨æˆ·ï¼ˆä½†åªæœ‰åœ¨å·²å½’æ¡£åæ‰æ¸…ç†ï¼‰
        for user_id in inactive_users:
            if self.unsaved_msg_count.get(user_id, 0) == 0:
                self.last_chat_time.pop(user_id, None)
                self.unsaved_msg_count.pop(user_id, None)
        
        # å¦‚æœç”¨æˆ·æ•°ä»ç„¶è¿‡å¤šï¼ŒæŒ‰æœ€åæ´»è·ƒæ—¶é—´æ’åºï¼Œä¿ç•™æœ€è¿‘çš„
        if len(self.last_chat_time) > self._max_inactive_users:
            sorted_users = sorted(self.last_chat_time.items(), key=lambda x: x[1], reverse=True)
            users_to_keep = set(u[0] for u in sorted_users[:self._max_inactive_users])
            
            for user_id in list(self.last_chat_time.keys()):
                if user_id not in users_to_keep and self.unsaved_msg_count.get(user_id, 0) == 0:
                    self.last_chat_time.pop(user_id, None)
                    self.unsaved_msg_count.pop(user_id, None)

    def _get_profile_path(self, user_id):
        return os.path.join(self.profiles_dir, f"{user_id}.json")

    @staticmethod
    def _is_valid_message_content(content: str) -> bool:
        """
        ç»Ÿä¸€çš„æ¶ˆæ¯å†…å®¹è¿‡æ»¤é€»è¾‘ï¼Œç”¨äºåˆ¤æ–­æ¶ˆæ¯æ˜¯å¦åº”è¢«çº³å…¥å½’æ¡£/æ£€ç´¢ã€‚
        
        è¿‡æ»¤è§„åˆ™ï¼š
        1. ä»¥å¸¸è§æŒ‡ä»¤å‰ç¼€å¼€å¤´çš„æ¶ˆæ¯
        2. å¸¦ä¸‹åˆ’çº¿ä¸”æ— ç©ºæ ¼çš„å†…éƒ¨æŒ‡ä»¤
        3. ä¸­æ–‡å­—ç¬¦ä¸è¶³2ä¸ªä¸”æ€»é•¿åº¦ä¸è¶³10çš„çŸ­æ¶ˆæ¯
        
        è¿”å› True è¡¨ç¤ºæ¶ˆæ¯æœ‰æ•ˆï¼ŒFalse è¡¨ç¤ºåº”è¢«è¿‡æ»¤ã€‚
        """
        import re
        content = content.strip()
        
        # 1. è¿‡æ»¤ä»¥å¸¸è§æŒ‡ä»¤å‰ç¼€å¼€å¤´çš„æ¶ˆæ¯
        if content.startswith(('/', '#', '~', '!', 'ï¼', 'ï¼', '&', '*')):
            return False
        
        # 2. ä¸“é—¨æ¸…æ´—å¸¦ä¸‹åˆ’çº¿çš„å†…éƒ¨æŒ‡ä»¤
        if "_" in content and " " not in content:
            return False
        
        # 3. ç»Ÿè®¡ä¸­æ–‡æ•°é‡æˆ–æ£€æŸ¥æ€»é•¿åº¦
        chinese_chars = _CHINESE_PATTERN.findall(content)
        if len(chinese_chars) < 2 and len(content) < 10:
            return False
        
        return True

    async def get_user_profile(self, user_id):
        """è·å–ç”¨æˆ·ç”»åƒ"""
        loop = asyncio.get_event_loop()
        path = self._get_profile_path(user_id)
        
        def _read():
            if not os.path.exists(path):
                # æ–°çš„ã€æ›´å…·ä½“çš„ç”»åƒç»“æ„
                return {
                    "basic_info": {
                        "qq_id": user_id,
                        "nickname": "æœªçŸ¥",
                        "gender": "æœªçŸ¥",
                        "age": "æœªçŸ¥",
                        "location": "æœªçŸ¥",
                        "job": "æœªçŸ¥",
                        "avatar_url": "",
                        "birthday": "æœªçŸ¥",
                        "constellation": "æœªçŸ¥",
                        "zodiac": "æœªçŸ¥",
                        "signature": "æš‚æ— ä¸ªæ€§ç­¾å"
                    },
                    "attributes": {
                        "personality_tags": [], # ä¾‹å¦‚ï¼šä¸¥è°¨ã€å¹½é»˜ (ä»…å½“æ˜æ˜¾è¡¨ç°æ—¶)
                        "hobbies": [],          # ä¾‹å¦‚ï¼šç¼–ç¨‹ã€çœ‹ç”µå½±
                        "skills": []            # ä¾‹å¦‚ï¼šPythonã€é’¢ç´
                    },
                    "preferences": {
                        "likes": [],            # æ˜ç¡®å–œæ¬¢çš„ï¼šç”Ÿæ¤°æ‹¿é“
                        "dislikes": []          # æ˜ç¡®è®¨åŒçš„ï¼šç¾å¼
                    },
                    "social_graph": {
                        "relationship_status": "åˆè¯†", # å½“å‰ä¸ AI çš„å…³ç³»ï¼šé™Œç”Ÿ -> ç†Ÿæ‚‰ -> ä¾èµ–
                        "important_people": []   # æåˆ°çš„æœ‹å‹/å®¶äºº
                    },
                    "dev_metadata": {           # ä¸“é—¨ä¸ºå¼€å‘è€…ä¿ç•™çš„å…ƒæ•°æ®
                        "os": [],
                        "tech_stack": []
                    }
                }
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        
        return await loop.run_in_executor(self.executor, _read)

    async def update_user_profile(self, user_id, update_data):
        """æ›´æ–°ç”¨æˆ·ç”»åƒ (Sidecar æ¨¡å¼)"""
        if not update_data:
            return
            
        loop = asyncio.get_event_loop()
        path = self._get_profile_path(user_id)
        
        def _update():
            profile = {}
            if os.path.exists(path):
                try:
                    with open(path, 'r', encoding='utf-8') as f:
                        profile = json.load(f)
                except:
                    pass
            
            # åˆå¹¶é€»è¾‘
            for key, value in update_data.items():
                if isinstance(value, list):
                    # åˆ—è¡¨å¤„ç†ï¼šå»é‡å¹¶åˆå¹¶
                    old_list = profile.get(key, [])
                    if not isinstance(old_list, list): old_list = [old_list]
                    new_list = list(set(old_list + value))
                    profile[key] = new_list
                elif isinstance(value, dict):
                    # å­—å…¸å¤„ç†ï¼šé€’å½’ä¸€çº§åˆå¹¶
                    old_dict = profile.get(key, {})
                    if not isinstance(old_dict, dict): old_dict = {}
                    old_dict.update(value)
                    profile[key] = old_dict
                else:
                    # åŸºæœ¬å±æ€§ï¼šç›´æ¥è¦†ç›–
                    profile[key] = value
            
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(profile, f, ensure_ascii=False, indent=4)
            return profile

        return await loop.run_in_executor(self.executor, _update)

    async def clear_user_profile(self, user_id):
        """æ¸…é™¤ç”¨æˆ·ç”»åƒ"""
        loop = asyncio.get_event_loop()
        path = self._get_profile_path(user_id)
        def _delete():
            if os.path.exists(path):
                os.remove(path)
        await loop.run_in_executor(self.executor, _delete)

    async def record_message(self, user_id, session_id, role, content, msg_type="text", user_name=None):
        import datetime
        msg_uuid = str(uuid.uuid4())
        
        # å¼‚æ­¥ä¿å­˜åˆ° SQLite
        loop = asyncio.get_event_loop()
        params = {
            "uuid": msg_uuid,
            "session_id": session_id,
            "user_id": user_id,
            "user_name": user_name,
            "role": role,
            "content": content,
            "msg_type": msg_type,
            "timestamp": datetime.datetime.now()
        }
        await loop.run_in_executor(self.executor, lambda: self.db.save_raw_memory(**params))
        
        # æ›´æ–°è®°å½•
        if role == "user":
            self.last_chat_time[user_id] = datetime.datetime.now().timestamp()
            self.unsaved_msg_count[user_id] = self.unsaved_msg_count.get(user_id, 0) + 1

    async def check_and_summarize(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œç§èŠå½’æ¡£ï¼ˆç”»åƒæ›´æ–°ç”±ç‹¬ç«‹è°ƒåº¦å™¨å¤„ç†ï¼‰"""
        import datetime
        now_ts = datetime.datetime.now().timestamp()
        timeout = self.config.get("private_memory_timeout", 1800)
        min_count = self.config.get("min_msg_count", 3)
        
        for user_id, last_time in list(self.last_chat_time.items()):
            if now_ts - last_time > timeout and self.unsaved_msg_count.get(user_id, 0) >= min_count:
                # è§¦å‘è®°å¿†å½’æ¡£
                await self._summarize_private_chat(user_id)
                self.unsaved_msg_count[user_id] = 0
        
        # å®šæœŸæ¸…ç†ä¸æ´»è·ƒç”¨æˆ·ç¼“å­˜ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
        self._cleanup_inactive_users()

    async def _update_persona_daily(self, user_id):
        """æ¯æ—¥ç”»åƒæ·±åº¦æ›´æ–° (ç”¨æˆ·ç”»åƒæ¶æ„)"""
        # 1. è·å–è¯¥ç”¨æˆ·å½“å¤©çš„æ‰€æœ‰è®°å¿†ç´¢å¼•
        import datetime
        today = datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        
        loop = asyncio.get_event_loop()
        memories = await loop.run_in_executor(self.executor, self.db.get_memories_since, user_id, today)
        
        if not memories:
            return

        # 2. ç»“åˆç°æœ‰ç”»åƒå’Œä»Šæ—¥è®°å¿†è¿›è¡Œæ·±åº¦æ›´æ–°
        current_persona = await self.get_user_profile(user_id)
        memory_texts = "\n".join([f"- {m.summary}" for m in memories])
        
        # å…¨æ–°çš„ Promptï¼Œå¼ºè°ƒäº‹å®æå–
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ã€ç”¨æˆ·ä¿¡æ¯æ¡£æ¡ˆå‘˜ã€‘ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä»Šæ—¥çš„æ–°å¢è®°å¿†ï¼Œæ›´æ–°ç”¨æˆ·çš„æ¡£æ¡ˆæ•°æ®ã€‚

ã€å½“å‰æ¡£æ¡ˆã€‘ï¼š
{json.dumps(current_persona, ensure_ascii=False, indent=2)}

ã€ä»Šæ—¥æ–°å¢è®°å¿†ã€‘ï¼š
{memory_texts}

ã€æ›´æ–°è§„åˆ™ã€‘ï¼š
1. **ç»å¯¹å®¢è§‚**ï¼šä»…ä»ã€ä»Šæ—¥æ–°å¢è®°å¿†ã€‘ä¸­æå–æ˜ç¡®çš„äº‹å®ã€‚ä¸è¦è¿›è¡Œå¿ƒç†åˆ†æï¼Œä¸è¦è„‘è¡¥ç”¨æˆ·æ²¡è¯´è¿‡çš„è¯ã€‚
2. **å¢é‡æ›´æ–°**ï¼š
   - å¦‚æœè®°å¿†ä¸­æ²¡æœ‰æåˆ°æŸé¡¹ä¿¡æ¯ï¼ˆå¦‚æ‰€åœ¨åœ°ã€èŒä¸šï¼‰ï¼Œè¯·ä¿æŒã€å½“å‰æ¡£æ¡ˆã€‘ä¸­çš„åŸå€¼ï¼Œ**ä¸è¦**å°†å…¶è¦†ç›–ä¸º"æœªçŸ¥"æˆ–nullã€‚
   - å¦‚æœæœ‰æ–°ä¿¡æ¯å†²çªï¼Œä»¥ã€ä»Šæ—¥æ–°å¢è®°å¿†ã€‘ä¸ºå‡†ã€‚
   - åˆ—è¡¨ç±»å‹ï¼ˆå¦‚ hobbies, likesï¼‰è¯·è¿½åŠ æ–°å†…å®¹ï¼Œå¹¶å»é‡ã€‚
3. **å­—æ®µå®šä¹‰**ï¼š
   - basic_info: ä»…æ›´æ–° gender(æ€§åˆ«), age(å¹´é¾„), location(æ‰€åœ¨åœ°), job(èŒä¸š)ã€‚
   - attributes: hobbies(å…·ä½“çˆ±å¥½), skills(æŠ€èƒ½), personality_tags(æ€§æ ¼å…³é”®è¯ï¼Œå¦‚"æ€¥èº","æ¸©å’Œ")ã€‚
   - preferences: likes(å–œæ¬¢çš„é£Ÿç‰©/äº‹ç‰©), dislikes(è®¨åŒçš„)ã€‚
   - social_graph: relationship_status(æ¨æµ‹å½“å‰ä¸AIçš„å…³ç³»é˜¶æ®µï¼Œå¦‚: å¼€å‘è€…ä¸æµ‹è¯•å‘˜/æœ‹å‹/æ­æ¡£)ã€‚
   - dev_metadata: å¦‚æœç”¨æˆ·æåŠä»£ç ã€æŠ€æœ¯æ ˆã€æ“ä½œç³»ç»Ÿï¼Œå­˜å…¥ tech_stackã€‚

ã€è¾“å‡ºè¦æ±‚ã€‘ï¼š
è¯·ç›´æ¥è¿”å›æ›´æ–°åçš„å®Œæ•´ JSON æ•°æ®ã€‚ä¸è¦åŒ…å« Markdown æ ‡è®°ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚
"""
        try:
            # è·å–æŒ‡å®šçš„æ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
            persona_model = self.config.get("persona_model", "").strip()
            if persona_model:
                provider = self.context.get_provider_by_id(persona_model)
                if not provider:
                    provider = self.context.get_using_provider()
            else:
                provider = self.context.get_using_provider()

            if not provider:
                return

            resp = await provider.text_chat(prompt=prompt)
            content = resp.completion_text
            
            # è§£æå¹¶ä¿å­˜
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "{" in content:
                content = content[content.find("{"):content.rfind("}")+1]
                
            new_persona = json.loads(content)
            
            # å†™å…¥æ–‡ä»¶
            path = self._get_profile_path(user_id)
            def _write():
                with open(path, 'w', encoding='utf-8') as f:
                    json.dump(new_persona, f, ensure_ascii=False, indent=4)
            await loop.run_in_executor(self.executor, _write)
            
        except Exception as e:
            logger.error(f"Daily persona update error: {e}")

    async def _summarize_private_chat(self, user_id):
        """å¯¹ç§èŠè¿›è¡Œæ€»ç»“å¹¶å­˜å…¥é•¿æœŸè®°å¿†ï¼ˆæŒ‰å¤©åˆ†ç»„å¤„ç†ï¼‰"""
        import datetime
        from itertools import groupby
        
        # 1. è·å–æœªå½’æ¡£çš„åŸå§‹æ¶ˆæ¯
        loop = asyncio.get_event_loop()
        # è·å–æ‰€æœ‰æœªå½’æ¡£æ¶ˆæ¯ï¼Œä¸è®¾é™åˆ¶
        # ä½¿ç”¨ lambda ä¼ é€’å‚æ•°ä»¥é¿å… run_in_executor çš„å…³é”®å­—å‚æ•°é™åˆ¶
        raw_msgs = await loop.run_in_executor(self.executor, lambda: self.db.get_unarchived_raw(user_id, limit=None))
        if not raw_msgs:
            return
        
        # æŒ‰æ—¶é—´æ­£åºæ’åˆ—ï¼ˆæ•°æ®åº“è¿”å›çš„æ˜¯å€’åºï¼‰
        raw_msgs.reverse()
        
        # è®¡ç®—å›æº¯æˆªæ­¢æ—¶é—´
        max_days = self.config.get("max_history_days", 0)
        cutoff_date = None
        if max_days > 0:
            cutoff_date = (datetime.datetime.now() - datetime.timedelta(days=max_days)).date()
        
        # æŒ‰æ—¥æœŸåˆ†ç»„
        def get_date_key(m):
            return m.timestamp.date()
            
        for date_key, group in groupby(raw_msgs, key=get_date_key):
            # å°† group è½¬ä¸ºåˆ—è¡¨ï¼Œå› ä¸º groupby çš„è¿­ä»£å™¨åªèƒ½ç”¨ä¸€æ¬¡
            group_msgs = list(group)
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡å›æº¯å¤©æ•°é™åˆ¶
            if cutoff_date and date_key < cutoff_date:
                # è¶…è¿‡é™åˆ¶ï¼Œç›´æ¥æ ‡è®°ä¸ºå·²å½’æ¡£ï¼Œä¸è¿›è¡Œæ€»ç»“
                ref_uuids = [m.uuid for m in group_msgs]
                await loop.run_in_executor(self.executor, self.db.mark_as_archived, ref_uuids)
                continue
                
            await self._process_single_summary_batch(user_id, group_msgs, date_key)

    async def _process_single_summary_batch(self, user_id, raw_msgs, date_key):
        """å¤„ç†å•æ‰¹æ¬¡ï¼ˆå•æ—¥ï¼‰æ¶ˆæ¯çš„æ€»ç»“"""
        import datetime
        
        # ä½¿ç”¨å…¬å…±è¿‡æ»¤æ–¹æ³•
        filtered_msgs = [m for m in raw_msgs if self._is_valid_message_content(m.content)]
        
        loop = asyncio.get_event_loop()
        
        if not filtered_msgs:
            # å¦‚æœæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ¶ˆæ¯ï¼Œä¹Ÿæ ‡è®°åŸæœ¬çš„æ‰€æœ‰æ¶ˆæ¯ä¸ºå·²å½’æ¡£
            ref_uuids = [m.uuid for m in raw_msgs]
            await loop.run_in_executor(self.executor, self.db.mark_as_archived, ref_uuids)
            return

        # æ„é€ å¯¹è¯æ–‡æœ¬
        chat_lines = [f"ã€æ—¥æœŸï¼š{date_key.strftime('%Y-%m-%d')}ã€‘"]
        for m in filtered_msgs:
            time_str = m.timestamp.strftime("%H:%M")
            name = m.user_name if m.role == "user" and m.user_name else m.role
            chat_lines.append(f"[{time_str}] {name}: {m.content}")
        chat_text = "\n".join(chat_lines)
        
        # 2. è°ƒç”¨ LLM æ€»ç»“
        # ä»é…ç½®è·å–æç¤ºè¯æ¨¡æ¿å¹¶æ›¿æ¢å ä½ç¬¦
        custom_prompt = self.config.get("summarize_prompt")
        ai_name = self.config.get("ai_name")
        prompt = custom_prompt.replace("{{chat_text}}", chat_text).replace("{{ai_name}}", ai_name)
        
        max_retries = 3
        retry_delay = 2
        full_content = ""
        
        for attempt in range(max_retries):
            try:
                # è·å–æŒ‡å®šçš„æ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
                summarize_model = self.config.get("summarize_model", "").strip()
                if summarize_model:
                    provider = self.context.get_provider_by_id(summarize_model)
                    if not provider:
                        provider = self.context.get_using_provider()
                else:
                    provider = self.context.get_using_provider()

                if not provider:
                    break
                    
                resp = await provider.text_chat(prompt=prompt)
                full_content = resp.completion_text
                
                if full_content and len(full_content) >= 5:
                    break # æˆåŠŸè·å–æ€»ç»“
                
                logger.warning(f"Summarization attempt {attempt + 1} produced empty or too short result.")
            except Exception as e:
                logger.error(f"Summarization attempt {attempt + 1} error: {e}")
            
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
        
        if not full_content or len(full_content) < 5:
            logger.error(f"Failed to summarize chat for user {user_id} after {max_retries} attempts.")
            return

        # è§£ææ—¥è®°å’Œç”»åƒ
        summary = full_content
        persona_update = {}
        if "[JSON_START]" in full_content and "[JSON_END]" in full_content:
            try:
                summary = full_content.split("[JSON_START]")[0].strip()
                json_str = full_content.split("[JSON_START]")[1].split("[JSON_END]")[0].strip()
                persona_update = json.loads(json_str)
                # å®æ—¶æ›´æ–°ç”»åƒ
                if persona_update:
                    await self.update_user_profile(user_id, persona_update)
            except Exception as e:
                logger.error(f"Failed to parse persona update: {e}")
            
        try:
            # ç¡®ä¿ ChromaDB å·²åˆå§‹åŒ–
            await self._ensure_chroma_initialized()
            
            # 3. å­˜å…¥ ChromaDB å’Œ SQLite Index
            index_id = str(uuid.uuid4())
            ref_uuids = [m.uuid for m in raw_msgs] # æ³¨æ„ï¼šå½’æ¡£æ ‡è®°åŸå§‹çš„æ‰€æœ‰æ¶ˆæ¯
            
            # ä½¿ç”¨è¯¥æ‰¹æ¬¡æœ€åä¸€æ¡æ¶ˆæ¯çš„æ—¶é—´ä½œä¸ºå½’æ¡£æ—¶é—´ï¼Œç¡®ä¿å†å²é‡æ„æ—¶çš„é¡ºåºæ­£ç¡®
            created_at = raw_msgs[-1].timestamp
            
            # è·å–å‰ä¸€æ¡è®°å¿†ç´¢å¼•ï¼Œå½¢æˆé“¾è¡¨ï¼ˆæ—¶é—´çº¿ï¼‰
            last_index = await loop.run_in_executor(self.executor, self.db.get_last_memory_index, user_id)
            prev_index_id = last_index.index_id if last_index else None
            
            # å‘é‡åŒ–å­˜å‚¨ï¼ˆä½¿ç”¨é…ç½®çš„ AI åç§°ï¼‰
            ai_name = self.config.get("ai_name", "åŠ©æ‰‹")
            add_params = {
                "ids": [index_id],
                "documents": [summary],
                "metadatas": [{
                    "user_id": user_id,
                    "source_type": "private",
                    "created_at": created_at.strftime("%Y-%m-%d %H:%M:%S"),
                    "ai_name": ai_name
                }]
            }
            await loop.run_in_executor(self.executor, lambda: self.collection.add(**add_params))
            
            # ç´¢å¼•å­˜å‚¨
            index_params = {
                "index_id": index_id,
                "summary": summary,
                "ref_uuids": json.dumps(ref_uuids),
                "prev_index_id": prev_index_id, # é“¾æ¥åˆ°å‰ä¸€æ¡
                "source_type": "private",
                "user_id": user_id,
                "created_at": created_at
            }
            await loop.run_in_executor(self.executor, lambda: self.db.save_memory_index(**index_params))
            
            # 4. æ ‡è®°è¿™äº›æ¶ˆæ¯ä¸ºå·²å½’æ¡£ï¼Œé˜²æ­¢é‡å¤æ€»ç»“
            await loop.run_in_executor(self.executor, self.db.mark_as_archived, ref_uuids)
            
        except Exception as e:
            logger.error(f"Save summarization error: {e}")

    async def retrieve_memories(self, user_id, query, limit=3):
        """æ£€ç´¢ç›¸å…³è®°å¿†å¹¶è¿”å›åŸæ–‡æ‘˜è¦åŠèƒŒæ™¯ï¼ˆåŸºäºæ—¶é—´é“¾ï¼‰ï¼Œä½¿ç”¨å…³é”®è¯é‡æ’åºæå‡ç²¾ç¡®åŒ¹é…"""
        # ç¡®ä¿ ChromaDB å·²åˆå§‹åŒ–
        await self._ensure_chroma_initialized()
        
        loop = asyncio.get_event_loop()
        
        # 1. ChromaDB æ£€ç´¢ï¼ˆå¤šå–ä¸€äº›ç»“æœä»¥ä¾¿è¿‡æ»¤å’Œé‡æ’åºåä»æœ‰è¶³å¤Ÿæ•°æ®ï¼‰
        query_params = {
            "query_texts": [query],
            "n_results": min(limit * 3, 15),  # å¤šå–ç»“æœä»¥ä¾¿é‡æ’åº
            "where": {"user_id": user_id}
        }
        results = await loop.run_in_executor(self.executor, lambda: self.collection.query(**query_params))
        
        if not results or not results['ids'] or not results['ids'][0]:
            return []
        
        # è·å–é…ç½®
        similarity_threshold = self.config.get("memory_similarity_threshold", 1.5)
        show_relevance_score = self.config.get("show_relevance_score", True)
        enable_keyword_boost = self.config.get("enable_keyword_boost", True)
        
        # è§£æå…³é”®è¯æƒé‡ï¼ˆæ–°æ ¼å¼ç›´æ¥æ˜¯æ•°å€¼å­—ç¬¦ä¸² "0.5"ï¼‰
        weight_config = self.config.get("keyword_boost_weight", "0.5")
        try:
            keyword_boost_weight = float(weight_config)
        except (ValueError, TypeError):
            # å‘åå…¼å®¹æ—§æ ¼å¼ "å‡è¡¡æ¨¡å¼ (0.5)"
            import re
            match = re.search(r'\(([\d.]+)\)', str(weight_config))
            keyword_boost_weight = float(match.group(1)) if match else 0.5
        
        # 2. é¢„å¤„ç†ç»“æœå¹¶è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
        distances = results.get('distances', [[]])[0] if 'distances' in results else []
        memory_data = []
        
        # æå–æŸ¥è¯¢å…³é”®è¯ï¼ˆç®€å•åˆ†è¯ï¼šæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²ï¼‰
        query_keywords = set()
        for char in ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€', ' ', ',', '.', '!', '?']:
            query = query.replace(char, ' ')
        query_keywords = set([w.strip().lower() for w in query.split() if len(w.strip()) > 0])
        
        for i in range(len(results['ids'][0])):
            distance = distances[i] if distances and i < len(distances) else float('inf')
            
            # è¿‡æ»¤ä½ç›¸å…³æ€§ç»“æœ
            if distance > similarity_threshold:
                logger.debug(f"Skipping memory with distance {distance:.3f} (threshold: {similarity_threshold})")
                continue
            
            index_id = results['ids'][0][i]
            summary = results['documents'][0][i]
            metadata = results['metadatas'][0][i]
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦ï¼ˆå…³é”®è¯åœ¨summaryä¸­å‡ºç°çš„æ¬¡æ•°ï¼‰
            keyword_score = 0
            summary_lower = summary.lower()
            for keyword in query_keywords:
                # ç²¾ç¡®åŒ¹é…å¾—åˆ†æ›´é«˜
                if keyword in summary_lower:
                    # ç»Ÿè®¡å‡ºç°æ¬¡æ•°
                    count = summary_lower.count(keyword)
                    keyword_score += count * len(keyword)  # é•¿å…³é”®è¯æƒé‡æ›´é«˜
            
            # å½’ä¸€åŒ–å…³é”®è¯å¾—åˆ†ï¼ˆ0-1ä¹‹é—´ï¼‰
            keyword_score_normalized = min(1.0, keyword_score / max(1, len(query) * 2))
            
            memory_data.append({
                'index_id': index_id,
                'summary': summary,
                'metadata': metadata,
                'distance': distance,
                'keyword_score': keyword_score_normalized
            })
        
        # 3. æ··åˆæ’åºï¼šç»“åˆå‘é‡ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…åº¦
        if enable_keyword_boost and query_keywords:
            # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆè·ç¦»è¶Šå°è¶Šå¥½ï¼Œå…³é”®è¯å¾—åˆ†è¶Šé«˜è¶Šå¥½ï¼‰
            for data in memory_data:
                # å‘é‡å¾—åˆ†ï¼šå°†è·ç¦»è½¬æ¢ä¸º0-1çš„å¾—åˆ†ï¼ˆè·ç¦»è¶Šå°å¾—åˆ†è¶Šé«˜ï¼‰
                vector_score = max(0, 1 - data['distance'] / 2.0)
                
                # ç»¼åˆå¾—åˆ† = å‘é‡å¾—åˆ† * (1 - weight) + å…³é”®è¯å¾—åˆ† * weight
                data['combined_score'] = (
                    vector_score * (1 - keyword_boost_weight) +
                    data['keyword_score'] * keyword_boost_weight
                )
            
            # æŒ‰ç»¼åˆå¾—åˆ†æ’åºï¼ˆå¾—åˆ†è¶Šé«˜è¶Šé å‰ï¼‰
            memory_data.sort(key=lambda x: x['combined_score'], reverse=True)
        else:
            # ä»…æŒ‰å‘é‡è·ç¦»æ’åº
            memory_data.sort(key=lambda x: x['distance'])
        
        # 4. åªä¿ç•™å‰ limit æ¡
        memory_data = memory_data[:limit]
        
        # 5. æ„é€ å¸¦æ—¶é—´çº¿èƒŒæ™¯å’Œè¯„åˆ†çš„è®°å¿†æ–‡æœ¬
        all_memories = []
        
        for data in memory_data:
            index_id = data['index_id']
            summary = data['summary']
            metadata = data['metadata']
            distance = data['distance']
            keyword_score = data.get('keyword_score', 0)
            created_at = metadata.get("created_at", "æœªçŸ¥æ—¶é—´")
            
            # è®¡ç®—æ˜¾ç¤ºçš„ç›¸å…³æ€§ç™¾åˆ†æ¯”
            if enable_keyword_boost and query_keywords:
                # ä½¿ç”¨ç»¼åˆå¾—åˆ†
                relevance_percent = int(data['combined_score'] * 100)
            else:
                # ä½¿ç”¨å‘é‡å¾—åˆ†
                relevance_percent = max(0, min(100, int((1 - distance / 2.0) * 100)))
            
            # å°è¯•é€šè¿‡é“¾è¡¨è·å–"å‰æƒ…æè¦"
            context_hint = ""
            db_index = await loop.run_in_executor(self.executor, self.db.get_memory_index_by_id, index_id)
            if db_index and db_index.prev_index_id:
                prev_index = await loop.run_in_executor(self.executor, self.db.get_memory_index_by_id, db_index.prev_index_id)
                if prev_index:
                    context_hint = f"ï¼ˆå‰æƒ…æè¦ï¼š{prev_index.summary[:50]}...ï¼‰"
            
            # è·å–åŸæ–‡ UUID åˆ—è¡¨
            raw_preview = ""
            if db_index and db_index.ref_uuids:
                uuids = json.loads(db_index.ref_uuids)
                # è·å–è¯¥æ€»ç»“å¯¹åº”çš„æ‰€æœ‰åŸæ–‡
                raw_msgs = await loop.run_in_executor(self.executor, self.db.get_memories_by_uuids, uuids)
                
                # ä½¿ç”¨å…¬å…±è¿‡æ»¤æ–¹æ³•ï¼Œå–å‰ 3 æ¡æœ‰æ•ˆåŸæ–‡ä½œä¸ºè¯æ®å‚è€ƒ
                filtered_raw = [
                    m.content[:30] for m in raw_msgs
                    if self._is_valid_message_content(m.content)
                ][:3]
                
                if filtered_raw:
                    raw_preview = "\n   â”” ğŸ“„ ç›¸å…³åŸæ–‡ï¼š" + " | ".join(filtered_raw)
            
            # æ·»åŠ  ID ä¿¡æ¯ï¼ˆUUID å‰ 8 ä½ï¼‰å’Œç›¸å…³æ€§è¯„åˆ†
            short_id = index_id[:8]
            
            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ˜¾ç¤ºç›¸å…³æ€§è¯„åˆ†
            if show_relevance_score:
                relevance_badge = f"ğŸ¯ {relevance_percent}% | "
            else:
                relevance_badge = ""
            
            all_memories.append(f"{relevance_badge}ğŸ†” {short_id} | â° {created_at}\nğŸ“ å½’æ¡£ï¼š{summary}{context_hint}{raw_preview}")
            
        return all_memories

    async def get_memory_detail(self, user_id, sequence_num):
        """è·å–æŒ‡å®šåºå·è®°å¿†çš„å®Œæ•´åŸæ–‡è¯¦æƒ…"""
        loop = asyncio.get_event_loop()
        
        # 1. è·å–æœ€è¿‘çš„ N æ¡è®°å¿†ï¼ˆä¸ºäº†æ‰¾åˆ°å¯¹åº”çš„åºå·ï¼‰
        # å‡è®¾ç”¨æˆ·è¾“å…¥çš„åºå·æ˜¯åŸºäº mem_list çš„ï¼ˆæœ€æ–°çš„ä¸º 1ï¼‰
        limit = sequence_num + 2
        memories = await loop.run_in_executor(self.executor, self.db.get_memory_list, user_id, limit)
        
        if not memories or len(memories) < sequence_num:
            return None, "æ‰¾ä¸åˆ°è¯¥åºå·çš„è®°å¿†ï¼Œè¯·ç¡®è®¤åºå·æ˜¯å¦å­˜åœ¨ã€‚"
            
        # 2. é”å®šç›®æ ‡è®°å¿†
        target_memory = memories[sequence_num - 1]
        
        # 3. è§£æåŸæ–‡ UUID
        if not target_memory.ref_uuids:
            return target_memory, []
            
        uuids = json.loads(target_memory.ref_uuids)
        raw_msgs = await loop.run_in_executor(self.executor, self.db.get_memories_by_uuids, uuids)
        
        return target_memory, raw_msgs
    
    async def delete_memory_by_sequence(self, user_id, sequence_num, delete_raw=False):
        """
        åˆ é™¤æŒ‡å®šåºå·çš„è®°å¿†ï¼ˆæ”¯æŒæ’¤é”€ï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            sequence_num: è®°å¿†åºå·ï¼ˆåŸºäº mem_list çš„åºå·ï¼Œæœ€æ–°çš„ä¸º 1ï¼‰
            delete_raw: æ˜¯å¦åŒæ—¶åˆ é™¤å…³è”çš„åŸå§‹æ¶ˆæ¯
            
        Returns:
            (success: bool, message: str, summary: str)
        """
        loop = asyncio.get_event_loop()
        
        # 1. è·å–ç›®æ ‡è®°å¿†
        limit = sequence_num + 2
        memories = await loop.run_in_executor(self.executor, self.db.get_memory_list, user_id, limit)
        
        if not memories or len(memories) < sequence_num:
            return False, "æ‰¾ä¸åˆ°è¯¥åºå·çš„è®°å¿†ï¼Œè¯·ç¡®è®¤åºå·æ˜¯å¦å­˜åœ¨ã€‚", ""
            
        target_memory = memories[sequence_num - 1]
        index_id = target_memory.index_id
        summary = target_memory.summary
        
        try:
            # ç¡®ä¿ ChromaDB å·²åˆå§‹åŒ–
            await self._ensure_chroma_initialized()
            
            # ä¿å­˜åˆ é™¤å‰çš„æ•°æ®ï¼ˆç”¨äºæ’¤é”€ï¼‰
            deleted_uuids = json.loads(target_memory.ref_uuids) if target_memory.ref_uuids else []
            
            # è·å–å‘é‡æ•°æ®ï¼ˆç”¨äºæ¢å¤ï¼‰
            vector_data = None
            try:
                chroma_result = await loop.run_in_executor(
                    self.executor,
                    lambda: self.collection.get(ids=[index_id], include=['embeddings', 'metadatas', 'documents'])
                )
                if chroma_result and chroma_result['ids']:
                    vector_data = {
                        'embedding': chroma_result['embeddings'][0] if chroma_result.get('embeddings') else None,
                        'metadata': chroma_result['metadatas'][0] if chroma_result.get('metadatas') else {},
                        'document': chroma_result['documents'][0] if chroma_result.get('documents') else summary
                    }
            except Exception as e:
                logger.debug(f"Failed to get vector data for backup: {e}")
            
            # åˆ›å»ºåˆ é™¤è®°å½•
            delete_record = {
                'index_id': index_id,
                'summary': summary,
                'ref_uuids': target_memory.ref_uuids,
                'prev_index_id': target_memory.prev_index_id,
                'source_type': target_memory.source_type,
                'user_id': user_id,
                'created_at': target_memory.created_at,
                'active_score': target_memory.active_score,
                'delete_raw': delete_raw,
                'deleted_uuids': deleted_uuids,
                'vector_data': vector_data
            }
            
            # ä¿å­˜åˆ°åˆ é™¤å†å²
            if user_id not in self._delete_history:
                self._delete_history[user_id] = []
            self._delete_history[user_id].insert(0, delete_record)
            # åªä¿ç•™æœ€è¿‘Næ¬¡åˆ é™¤
            self._delete_history[user_id] = self._delete_history[user_id][:self._max_undo_history]
            
            # 2. ä» ChromaDB åˆ é™¤å‘é‡æ•°æ®
            await loop.run_in_executor(self.executor, lambda: self.collection.delete(ids=[index_id]))
            
            # 3. å¦‚æœéœ€è¦ï¼Œåˆ é™¤å…³è”çš„åŸå§‹æ¶ˆæ¯
            if delete_raw and target_memory.ref_uuids:
                uuids = json.loads(target_memory.ref_uuids)
                await loop.run_in_executor(self.executor, self.db.delete_raw_memories_by_uuids, uuids)
            else:
                # ä¸åˆ é™¤åŸå§‹æ¶ˆæ¯æ—¶ï¼Œå°†å…¶æ ‡è®°ä¸ºæœªå½’æ¡£ï¼Œä»¥ä¾¿é‡æ–°æ€»ç»“
                if deleted_uuids:
                    def _mark_unarchived():
                        from .db_manager import RawMemory
                        with self.db.db.connection_context():
                            RawMemory.update(is_archived=False).where(RawMemory.uuid << deleted_uuids).execute()
                    await loop.run_in_executor(self.executor, _mark_unarchived)
            
            # 4. ä» SQLite åˆ é™¤è®°å¿†ç´¢å¼•
            await loop.run_in_executor(self.executor, self.db.delete_memory_index, index_id)
            
            return True, "åˆ é™¤æˆåŠŸ", summary
            
        except Exception as e:
            logger.error(f"Delete memory error: {e}")
            return False, f"åˆ é™¤å¤±è´¥ï¼š{e}", summary
    
    async def undo_last_delete(self, user_id):
        """
        æ’¤é”€æœ€è¿‘ä¸€æ¬¡åˆ é™¤æ“ä½œ
        
        Args:
            user_id: ç”¨æˆ·ID
            
        Returns:
            (success: bool, message: str, summary: str)
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ é™¤å†å²
        if user_id not in self._delete_history or not self._delete_history[user_id]:
            return False, "æ²¡æœ‰å¯æ’¤é”€çš„åˆ é™¤æ“ä½œã€‚", ""
        
        # è·å–æœ€è¿‘çš„åˆ é™¤è®°å½•
        delete_record = self._delete_history[user_id].pop(0)
        
        loop = asyncio.get_event_loop()
        
        try:
            # 1. æ¢å¤ SQLite ä¸­çš„è®°å¿†ç´¢å¼•
            index_params = {
                'index_id': delete_record['index_id'],
                'summary': delete_record['summary'],
                'ref_uuids': delete_record['ref_uuids'],
                'prev_index_id': delete_record['prev_index_id'],
                'source_type': delete_record['source_type'],
                'user_id': delete_record['user_id'],
                'created_at': delete_record['created_at'],
                'active_score': delete_record.get('active_score', 100)
            }
            await loop.run_in_executor(self.executor, lambda: self.db.save_memory_index(**index_params))
            
            # ç¡®ä¿ ChromaDB å·²åˆå§‹åŒ–
            await self._ensure_chroma_initialized()
            
            # 2. æ¢å¤ ChromaDB ä¸­çš„å‘é‡æ•°æ®
            vector_data = delete_record.get('vector_data')
            if vector_data and vector_data.get('embedding'):
                # æœ‰å®Œæ•´çš„å‘é‡æ•°æ®ï¼Œç›´æ¥æ¢å¤
                add_params = {
                    'ids': [delete_record['index_id']],
                    'documents': [vector_data.get('document', delete_record['summary'])],
                    'metadatas': [vector_data.get('metadata', {'user_id': user_id})],
                    'embeddings': [vector_data['embedding']]
                }
                await loop.run_in_executor(self.executor, lambda: self.collection.add(**add_params))
            else:
                # æ²¡æœ‰å‘é‡æ•°æ®ï¼Œé‡æ–°ç”Ÿæˆ
                add_params = {
                    'ids': [delete_record['index_id']],
                    'documents': [delete_record['summary']],
                    'metadatas': [{
                        'user_id': user_id,
                        'source_type': delete_record['source_type'],
                        'created_at': delete_record['created_at'].strftime("%Y-%m-%d %H:%M:%S") if hasattr(delete_record['created_at'], 'strftime') else str(delete_record['created_at'])
                    }]
                }
                await loop.run_in_executor(self.executor, lambda: self.collection.add(**add_params))
            
            # 3. æ¢å¤åŸå§‹æ¶ˆæ¯çš„å½’æ¡£çŠ¶æ€
            if delete_record['deleted_uuids']:
                def _mark_archived():
                    from .db_manager import RawMemory
                    with self.db.db.connection_context():
                        RawMemory.update(is_archived=True).where(
                            RawMemory.uuid << delete_record['deleted_uuids']
                        ).execute()
                try:
                    await loop.run_in_executor(self.executor, _mark_archived)
                except Exception as e:
                    logger.debug(f"Failed to restore raw messages archive status: {e}")
            
            return True, "æ’¤é”€æˆåŠŸ", delete_record['summary']
            
        except Exception as e:
            logger.error(f"Undo delete error: {e}")
            # æ¢å¤å¤±è´¥ï¼Œå°†è®°å½•æ”¾å›å†å²
            self._delete_history[user_id].insert(0, delete_record)
            return False, f"æ’¤é”€å¤±è´¥ï¼š{e}", delete_record['summary']
    
    async def delete_memory_by_id(self, user_id, short_id, delete_raw=False):
        """
        æ ¹æ®è®°å¿† IDï¼ˆçŸ­ ID æˆ–å®Œæ•´ UUIDï¼‰åˆ é™¤è®°å¿†
        
        Args:
            user_id: ç”¨æˆ·ID
            short_id: è®°å¿†IDï¼ˆå¯ä»¥æ˜¯å‰8ä½çŸ­IDæˆ–å®Œæ•´UUIDï¼‰
            delete_raw: æ˜¯å¦åŒæ—¶åˆ é™¤å…³è”çš„åŸå§‹æ¶ˆæ¯
            
        Returns:
            (success: bool, message: str, summary: str)
        """
        loop = asyncio.get_event_loop()
        
        # 1. æŸ¥æ‰¾åŒ¹é…çš„è®°å¿†ç´¢å¼•
        def _find_memory():
            with self.db.db.connection_context():
                from .db_manager import MemoryIndex
                # å¦‚æœæ˜¯çŸ­IDï¼ˆ8ä½ï¼‰ï¼ŒæŸ¥æ‰¾åŒ¹é…çš„å®Œæ•´UUID
                if len(short_id) == 8:
                    query = MemoryIndex.select().where(
                        (MemoryIndex.user_id == user_id) &
                        (MemoryIndex.index_id.startswith(short_id))
                    )
                else:
                    # å®Œæ•´UUID
                    query = MemoryIndex.select().where(
                        (MemoryIndex.user_id == user_id) &
                        (MemoryIndex.index_id == short_id)
                    )
                return query.first()
        
        try:
            target_memory = await loop.run_in_executor(self.executor, _find_memory)
            
            if not target_memory:
                return False, f"æ‰¾ä¸åˆ° ID ä¸º {short_id} çš„è®°å¿†ï¼Œè¯·ç¡®è®¤ ID æ˜¯å¦æ­£ç¡®ã€‚", ""
            
            index_id = target_memory.index_id
            summary = target_memory.summary
            
            # ç¡®ä¿ ChromaDB å·²åˆå§‹åŒ–
            await self._ensure_chroma_initialized()
            
            # 2. ä» ChromaDB åˆ é™¤å‘é‡æ•°æ®
            await loop.run_in_executor(self.executor, lambda: self.collection.delete(ids=[index_id]))
            
            # 3. å¦‚æœéœ€è¦ï¼Œåˆ é™¤å…³è”çš„åŸå§‹æ¶ˆæ¯
            if delete_raw and target_memory.ref_uuids:
                uuids = json.loads(target_memory.ref_uuids)
                await loop.run_in_executor(self.executor, self.db.delete_raw_memories_by_uuids, uuids)
            
            # 4. ä» SQLite åˆ é™¤è®°å¿†ç´¢å¼•
            await loop.run_in_executor(self.executor, self.db.delete_memory_index, index_id)
            
            return True, "åˆ é™¤æˆåŠŸ", summary
            
        except Exception as e:
            logger.error(f"Delete memory by ID error: {e}")
            return False, f"åˆ é™¤å¤±è´¥ï¼š{e}", ""
    
    async def export_raw_messages(self, user_id, format="jsonl", start_date=None, end_date=None, limit=None):
        """
        å¯¼å‡ºåŸå§‹æ¶ˆæ¯æ•°æ®ç”¨äºæ¨¡å‹å¾®è°ƒ
        
        Args:
            user_id: ç”¨æˆ·ID
            format: å¯¼å‡ºæ ¼å¼ (jsonl, json, txt)
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            limit: é™åˆ¶æ•°é‡
            
        Returns:
            (success: bool, data: str, stats: dict)
        """
        loop = asyncio.get_event_loop()
        
        try:
            # è·å–åŸå§‹æ¶ˆæ¯
            raw_msgs = await loop.run_in_executor(
                self.executor,
                self.db.get_all_raw_messages,
                user_id,
                start_date,
                end_date,
                limit
            )
            
            if not raw_msgs:
                return False, "æ²¡æœ‰æ‰¾åˆ°å¯å¯¼å‡ºçš„æ¶ˆæ¯", {}
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = await loop.run_in_executor(self.executor, self.db.get_message_stats, user_id)
            stats["exported"] = len(raw_msgs)
            
            # æ ¹æ®æ ¼å¼å¯¼å‡º
            if format == "jsonl":
                data = self._export_as_jsonl(raw_msgs)
            elif format == "json":
                data = self._export_as_json(raw_msgs)
            elif format == "txt":
                data = self._export_as_txt(raw_msgs)
            elif format == "alpaca":
                data = self._export_as_alpaca(raw_msgs)
            elif format == "sharegpt":
                data = self._export_as_sharegpt(raw_msgs)
            else:
                return False, f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼ï¼š{format}", {}
            
            return True, data, stats
            
        except Exception as e:
            logger.error(f"Export raw messages error: {e}")
            return False, f"å¯¼å‡ºå¤±è´¥ï¼š{e}", {}
    
    def _export_as_jsonl(self, raw_msgs):
        """å¯¼å‡ºä¸º JSONL æ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰"""
        lines = []
        for msg in raw_msgs:
            if not self._is_valid_message_content(msg.content):
                continue
            obj = {
                "role": "assistant" if msg.role == "assistant" else "user",
                "content": msg.content,
                "timestamp": msg.timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                "user_id": msg.user_id,
                "user_name": msg.user_name
            }
            lines.append(json.dumps(obj, ensure_ascii=False))
        return "\n".join(lines)
    
    def _export_as_json(self, raw_msgs):
        """å¯¼å‡ºä¸º JSON æ•°ç»„æ ¼å¼"""
        messages = []
        for msg in raw_msgs:
            if not self._is_valid_message_content(msg.content):
                continue
            messages.append({
                "role": "assistant" if msg.role == "assistant" else "user",
                "content": msg.content,
                "timestamp": msg.timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                "user_id": msg.user_id,
                "user_name": msg.user_name
            })
        return json.dumps(messages, ensure_ascii=False, indent=2)
    
    def _export_as_txt(self, raw_msgs):
        """å¯¼å‡ºä¸ºçº¯æ–‡æœ¬æ ¼å¼"""
        lines = []
        for msg in raw_msgs:
            if not self._is_valid_message_content(msg.content):
                continue
            role_name = "åŠ©æ‰‹" if msg.role == "assistant" else (msg.user_name or "ç”¨æˆ·")
            time_str = msg.timestamp.strftime("%Y-%m-%d %H:%M:%S")
            lines.append(f"[{time_str}] {role_name}: {msg.content}")
        return "\n".join(lines)
    
    def _export_as_alpaca(self, raw_msgs):
        """å¯¼å‡ºä¸º Alpaca æ ¼å¼ï¼ˆç”¨äºå¾®è°ƒï¼‰"""
        conversations = []
        current_instruction = None
        
        for msg in raw_msgs:
            if not self._is_valid_message_content(msg.content):
                continue
                
            if msg.role == "user":
                current_instruction = msg.content
            elif msg.role == "assistant" and current_instruction:
                conversations.append({
                    "instruction": current_instruction,
                    "input": "",
                    "output": msg.content
                })
                current_instruction = None
        
        return json.dumps(conversations, ensure_ascii=False, indent=2)
    
    async def export_all_users_messages(self, format="jsonl", start_date=None, end_date=None, limit=None):
        """
        å¯¼å‡ºæ‰€æœ‰ç”¨æˆ·çš„åŸå§‹æ¶ˆæ¯æ•°æ®
        
        Args:
            format: å¯¼å‡ºæ ¼å¼ (jsonl, json, txt, alpaca, sharegpt)
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            limit: é™åˆ¶æ•°é‡
            
        Returns:
            (success: bool, data: str, stats: dict)
        """
        loop = asyncio.get_event_loop()
        
        try:
            # è·å–æ‰€æœ‰ç”¨æˆ·çš„æ¶ˆæ¯
            raw_msgs = await loop.run_in_executor(
                self.executor,
                self.db.get_all_users_messages,
                start_date,
                end_date,
                limit
            )
            
            if not raw_msgs:
                return False, "æ²¡æœ‰æ‰¾åˆ°å¯å¯¼å‡ºçš„æ¶ˆæ¯", {}
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = await loop.run_in_executor(self.executor, self.db.get_all_users_stats)
            stats["exported"] = len(raw_msgs)
            
            # æ ¹æ®æ ¼å¼å¯¼å‡º
            if format == "jsonl":
                data = self._export_as_jsonl(raw_msgs)
            elif format == "json":
                data = self._export_as_json(raw_msgs)
            elif format == "txt":
                data = self._export_as_txt(raw_msgs)
            elif format == "alpaca":
                data = self._export_as_alpaca(raw_msgs)
            elif format == "sharegpt":
                data = self._export_as_sharegpt(raw_msgs)
            else:
                return False, f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼ï¼š{format}", {}
            
            return True, data, stats
            
        except Exception as e:
            logger.error(f"Export all users messages error: {e}")
            return False, f"å¯¼å‡ºå¤±è´¥ï¼š{e}", {}
    
    def _export_as_sharegpt(self, raw_msgs):
        """å¯¼å‡ºä¸º ShareGPT æ ¼å¼ï¼ˆç”¨äºå¾®è°ƒï¼‰"""
        conversations = []
        current_conversation = []
        
        for msg in raw_msgs:
            if not self._is_valid_message_content(msg.content):
                continue
            
            role = "gpt" if msg.role == "assistant" else "human"
            current_conversation.append({
                "from": role,
                "value": msg.content
            })
            
            # æ¯ä¸ªå¯¹è¯è½®æ¬¡ï¼ˆä¸€é—®ä¸€ç­”ï¼‰ä½œä¸ºä¸€ä¸ªå®Œæ•´å¯¹è¯
            if msg.role == "assistant" and len(current_conversation) >= 2:
                conversations.append({
                    "conversations": current_conversation.copy()
                })
                current_conversation = []
        
        return json.dumps(conversations, ensure_ascii=False, indent=2)
